\documentclass[a4paper,11pt,notitlepage]{report}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{bibgerm}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{color}
\usepackage{enumerate}
\usepackage{tabularx}
\usepackage{subfig}
\usepackage{fancyhdr}
\usepackage[pdftex,pdfpagelabels,colorlinks,backref,pagebackref]{hyperref}
\usepackage{tikz} % SELBST HINZUGEFÜGT
\usepackage{slashbox}
% == Set the heading style ===================================================
\setlength{\headheight}{14pt}
\pagestyle{fancyplain}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\rightmark}}
\rhead[\fancyplain{}{\leftmark}]{\fancyplain{}{\thepage}}
\cfoot{}
\renewcommand{\headrulewidth}{0.4pt}
% ============================================================================

% == Set correct values for fitting floats ===================================
\tolerance=2000
\emergencystretch=10pt

\setcounter{topnumber}{3}
\setcounter{totalnumber}{5}
\setcounter{bottomnumber}{2}

% To make those darn floats fit where they should
\setcounter{totalnumber}{9}
\setcounter{topnumber}{9}
\setcounter{bottomnumber}{9}
\renewcommand{\textfraction}{0.00}
\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
% ============================================================================

% == German definitions for theorems etc. ==================================== 
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Satz}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{corollary}{Korollar}[chapter]
\newtheorem{observation}{Beobachtung}[chapter]
\newtheorem{fact}{Fakt}[chapter]
\newtheorem{remark}{Bemerkung}[chapter]
\newtheorem{example}{Beispiel}[chapter]
% ============================================================================

% == Abkürzungen für die reellen, natürlichen, ganzen,... Zahlen =============
\newcommand{\R}{{\ensuremath{\mathbb{R}}}}
\newcommand{\N}{{\ensuremath{\mathbb{N}}}}
\newcommand{\Z}{{\ensuremath{\mathbb{Z}}}}
\newcommand{\C}{{\ensuremath{\mathbb{C}}}}
\newcommand{\Q}{{\ensuremath{\mathbb{Q}}}}
\newcommand{\F}{{\ensuremath{\mathbb{F}}}}
\newcommand{\Prim}{{\ensuremath{\mathbb{P}}}}
\newcommand{\E}{{\ensuremath{\mathbb{E}}}}
% ============================================================================

% == Makros für Autorenname und -adresse =====================================
\newcommand{\myaddress}[6]{%
  \parbox{\textwidth}{\textbf{\large #1}\\
    #2\\ #3\\ #4\\ 
    \ifthenelse{\equal{#5}{}}{}{Email: \href{mailto:#5}{\texttt{#5}}\\}
    \ifthenelse{\equal{#6}{}}{}{WWW: \href{#6}{\path|#6|}\\}
  } 
}

\newcommand{\myauthor}[1]{%
  \addtocontents{toc}{\protect\hspace{3.35ex}%
  \textsl{#1}\par}\vspace{-4ex}\quad\hfill\textsl{\Large #1}\vspace{8ex}}

\newcommand{\myname}[1]{\Large #1}

\title{\textbf{{Einführung in die Stochastik - Mitschrieb} \\[5ex] 
    {\Large Vorlesung im Wintersemester 2011/2012\\[5ex]}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tragen Sie in der folg. Zeile Ihren Namen ein: %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{\myname{Sarah Lutteropp}}

\begin{document}
\shorthandoff{"}
\maketitle
\setcounter{tocdepth}{1}
\tableofcontents

\section*{Vorwort}
Dies ist ein Mitschrieb der Vorlesung “Einführung in die Stochastik” vom Wintersemester 2011/2012 am Karlsruher Institut für Technologie, die von Herrn Prof. Dr. Günther Last gehalten wird.

\chapter{Deskriptive Statistik}
\section{Der Grundraum}
$\emptyset \neq \Omega$ = Grundraum (Grundgesamtheit, Merkmalsraum, Stichprobenraum)
Annahme: $\Omega$ ist diskret(endlich oder abzählbar unendlich) (Häufig $\Omega \subseteq \R$)

\section{Absolute und relative Häufigkeit}
$x_1, \ldots, x_n \in \Omega$ ("Daten") \newline
$h(\omega) = \text{card}\left\{j\in\{1, \ldots, n\} \colon x_j = \omega\right\}, \omega \in \Omega$, absolute Häufigkeit von $\omega$

\paragraph{Bemerkung}
$\sum\limits_{\omega \in \Omega}{h(\omega)} = n$

\paragraph{Definition}
$\frac{1}{n} h(\omega)$ = relative Häufigkeit von $\omega$ \newline
$h(A)=\text{card}\left\{j\in\{1,\ldots,n\}\colon x_j \in A\right\}, A \subset \Omega$ = absolute Häufigkeit von A, $\frac{1}{n} h(A)$ = relative Häufigkeit von A

\section{Histogramm}
$x_1, \ldots, x_n \in \R, b_1 < b_2 < \ldots < b_s$ mit $b_1 \leq \min\limits_{1 \leq i \leq n}{x_i}, b_s > \max\limits_{1 \leq i \leq n}{x_i}$
\newline
TODO: BILD
\newline
$d_j(b_{j+1}-b_j)=h([b_j,b_{j+1})) = \text{card} \left\{i\in\{1,\ldots,n\}\colon b_j \leq x_i < b_{j+1}\right\}$

\section{Lagemaße}
\paragraph{Definition}
Ein \textbf{Lagemaß} ist eine Abbildung $l \colon \R^n \rightarrow \R$ mit $$l(x_1+a,\ldots,x_n+a) = l(x_1,\ldots,x_n)+a$$ "Verschiebungskovarianz".
$x_1,\ldots,x_n,a \in \R$

\subsection{Arithmetisches Mittel}
$x_1,\ldots,x_n \in \R, \bar{x} := \frac{1}{n} \sum\limits_{j=1}^{n}{x_j}$ "Schwerpunkt der Daten"

\paragraph{Fakt}
$\sum\limits_{j=1}^{n}{(x_i - t)^2} \overset{t}{\rightarrow} \text{Min}$
\newline
Lösung: $t = \bar{x}$
\newline
"Prinzip der kleinsten Quadrate"

\paragraph{Beweis}
$\frac{1}{n} \sum\limits_{j=1}^{n}{(x_j - t)^2} = t^2 - 2\bar{x}t + \frac{1}{n} \sum\limits_{j=1}^{n}{x_j^2} = (t - \bar{x})^2 + \frac{1}{n} \sum\limits_{j=1}^{n}{x_j^2 - (\bar{x})^2}$

\subsection{Median, Quantile}
$x_1,\ldots,x_n \in \R \Rightarrow x_{(1)} \leq x_{(2)} \leq \ldots \leq x_{(n)}$ geordnete Stichprobe

\paragraph{Definition}

$$x_{1/2}:= \begin{cases}
	x_{(\frac{n+1}{2})} & \text{, falls } n \text{ ungerade} \\
	\frac{1}{2}(x_{(\frac{n}{2})} + x_{(\frac{n}{2}+1)}) & \text{, falls } n \text{ gerade}
\end{cases}
$$ 
heißt \textbf{Median} von $x_1,\ldots,x_n$.

\paragraph{Fakt}
$\sum\limits_{j=1}^{n}{|x_j - x_{1/2}|} = \min\limits_{t}{\sum\limits_{j=1}^{n}{|x_j - t|}}$ Übungsaufgabe

\paragraph{Bemerkung}
Der Median ist "robust" gegenüber "Ausreißern".
Ist etwa $x_1 = \ldots = x_9 = 1$ und $x_{10} = 1000 (n=10)$, so gilt $\bar{x} = 100,9  , x_{1/2} = 1$

\paragraph{Definition}
Für 0 < p < 1 heißt
$$
x_p := \begin{cases}
	x_{(\lfloor n \cdot p + 1 \rfloor )} & \text{, falls } n \cdot p \notin \N \\
	\frac{1}{2}(x_{(n \cdot p)} + x_{(n \cdot p + 1)}) & \text{, falls } n \cdot p \in \N
\end{cases}
$$
\textbf{p-Quantil} von $x_1, \ldots, x_n$.

\paragraph{Interpretation}
Mindestens $p \cdot 100 \%$ der Daten liegen links von $x_p$ und mindestens $(1-p) \cdot 100 \%$ liegen rechts von $x_p$. \newline
$x_{1/4}=$ unteres Quartil, $x_{3/4}=$ oberes Quartil

\section{Streuungsmaße}
\paragraph{Definition}
Eine Abbildung $\sigma \colon \R^n \rightarrow \R$ mit $$\sigma(x_1+a,\ldots,x_n+a) = \sigma(x_1,\ldots,x_n)\text{ (Translationsinvarianz)}$$ heißt \textbf{Streuungsmaß}.

\subsection{Empirische Varianz}
$s^2 := \frac{1}{n-1} \sum\limits_{j=1}^{n}{(x_j - \bar{x})^2}$ = \textbf{empirische Varianz} von $x_1,\ldots,x_n$

\subsection{Empirische Standardabweichung}
$s := + \sqrt{s^2}$ = \textbf{empirische Standardabweichung} von $x_1,\ldots,x_n$

\subsection{Spannweite}
$x_{(n)} - x_{(1)}$ = \textbf{Spannweite} von $x_1,\ldots,x_n$

\subsection{Quartilsabstand}
$x_{(3/4)} - x_{(1/4)}$ = \textbf{Quartilsabstand} von $x_1,\ldots,x_n$

\section{Empirischer Korrelationskoeffizient}

$(x_1,y_1), \ldots, (x_n,y_n) \in \R^2$
TODO: BILD

Gesucht: Gerade $y = a + b \cdot x$ so, dass
$$(*) \sum\limits_{j=1}^{n}{(y_j - a - b x_j)^2} \overset{a,b}\rightarrow \text{Min}$$

\paragraph{Definition}
$\sigma_{x}^2 = \frac{1}{n}\sum\limits_{j=1}^{n}{(x_j - \bar{x})^2}$
$\sigma_{y}^2 = \frac{1}{n}\sum\limits_{j=1}^{n}{(y_j - \bar{y})^2}$

$\sigma_{xy} = \frac{1}{n}\sum\limits_{j=1}^{n}{(x_j - \bar{x})(y_j - \bar{y})}$ \textbf{empirische Kovarianz} $\sigma_x^2 > 0, \sigma_y^2 >0.$

Lösung von (*):
$b^* = \frac{\sigma_{xy}}{\sigma_{x^2}}, a^*= \bar{y} - b^* \cdot \bar{x}$


$\min\limits_{a,b} {\sum\limits_{j=1}^{n}{(y_j - a - b x_j)^2}} \stackrel{!}{=} \min\limits_{b}{\sum\limits_{j=1}^{n}{(y_i - \bar{y} - b (x_j - \bar{x}))^2}}=\ldots$

"lineare Regression"
\newline

Einsetzen von $a^*$ und $b^*$ in die Zielfunktion:
$$0 \leq \sum\limits_{j=1}^{n}{(y_j - a^* - b^* x_j)^2} = \ldots = n \sigma_y^2 (1-(\frac{\sigma_{xy}}{\sigma_x \sigma_y})^2)$$

\paragraph{Definition}
$r_{xy}:= \frac{\sigma_{xy}}{\sigma_x \sigma_y}$ heißt \textbf{empirischer Korrelationskoeffizient} (\emph{Pearson}).

\paragraph{Folgerung}
$|r_{xy}|\leq 1$
\newline
Es gilt $r_{xy} = \pm 1 \Leftrightarrow$ Punktewolke liegt exakt auf der Geraden $y=a^*+b^*x$.
Dabei ist $b^* > 0$, falls $r_{xy} = 1$ und $b^* < 0$, falls $r_{xy} = -1$.
\newline 
\emph{Dieser empirische Korrelationskoeffizient ist ein Maß für die (affin) lineare Abhängigkeit zwischen den $x_j$ und den $y_j$.}

\chapter{Ereignisse und Zufallsvariablen}

\section{Definition}
Gegeben sei eine \underline{Grundmenge} $\Omega$. Die Elemente von $\Omega$ heißen \textbf{Elementarereignisse}. Teilmengen von $\Omega$ heißen \textbf{Ereignisse}. (Idee: $\omega \in \Omega$ ist Ausgang eines zufälligen Versuchs.)

\paragraph{Interpretation}
Ein Ereignis $A \subset \Omega$ "tritt ein", wenn $\omega \in A$.

\section{Beispiele}
\begin{itemize}
	\item (i) (Münzwurf) \newline
		$\Omega = \{0,1\} (\text{oder } \Omega = \{W,Z\})$
	\item (ii) (m Münzwürfe) \newline
		$\Omega = \{0,1\}^m (A = \{\omega = (\omega_1, \ldots, \omega_m) : \sum\limits_{j=1}^{m}{\omega_j} \geq k\} \text{ Ereignis })$
	\item (iii) Werfen von 2 Würfeln \newline
		$\Omega = \{1, \ldots, 6\}^2$
	\item (iv) Brownsche Bewegung \newline
		(TODO: BILD) Bewegung eines Blütenpollens in einer Flüssigkeit
		\newline
		$\Rightarrow$ Zukunftsmusik
		\newline
		$\Omega = C([0,1], \R^2)$
\end{itemize}

\section{Bemerkung (Mengentheoretische Operationen)}
Seien $A, B, A_1, A_2, \ldots \subset \Omega$.
\newline
$A \cap B = \{\omega \in \Omega : \omega \in A \text{ und } \omega \in B\} \mathop{\hat{=}}  \text{"A und B treten ein"}$
\newline
$A \cup B \mathop{\hat{=}}  \text{"A oder B treten ein"}$
\newline
$\bar{A} \equiv A^c := \Omega \backslash A = \{\omega \in \Omega : \omega \notin A \} \mathop{\hat{=}} \text{"A tritt nicht ein"}$
\newline
$A \backslash B = A \cap B^c \mathop{\hat{=}} \text{"A tritt ein, aber nicht B"}$
\newline
$A \subset B \mathop{\hat{=}} \text{"wenn A, dann B"}$
\newline
$\emptyset \mathop{\hat{=}} \text{"unmögliches Ereignis"}$
\newline
$\Omega \mathop{\hat{=}} \text{"sicheres Ereignis"}$
\newline
\paragraph{Abkürzung} $AB = A \cap B$

\section{Definition}
Eine Abbildung $X \colon \Omega \rightarrow \R$ heißt (reelle) \textbf{Zufallsvariable}. Für $\omega \in \Omega$ heißt $X(\omega)$ \textbf{Realisierung} der Zufallsvariable zu $\omega$.

\paragraph{Idee} 
Mit $\omega \in \Omega$ bekommt auch $X(\omega)$ einen zufälligen Charakter.

\paragraph{Definition}
$X^{-1} \colon \mathcal{P}(\R) \rightarrow \mathcal{P}(\Omega) = \{A \colon A \in \Omega\}$ ist definiert durch 
$$X^{-1}(A) = \{\omega \in \Omega \colon X(\omega) \in A\} \text{ ("Urbild von A unter X")}$$

\paragraph{Bemerkung}
\begin{itemize}
\item $X^{-1}(A \cap B) = X^{-1}(A) \cap X^{-1}(B), A,B \subset \R$
\item $X^{-1}(A \cup B) = X^{-1}(A) \cup X^{-1}(B)$
\item $X^{-1}(\bigcup\limits_{j=1}^{\infty}{A_j}) = \bigcup\limits_{j=1}^{\infty}{X^{-1}(A_j)}$
\item $X^{-1}(\bigcap\limits_{j=1}^{\infty}{A_j}) = \bigcap\limits_{j=1}^{\infty}{X^{-1}(A_j)}$
\end{itemize}

\paragraph{Vereinbarung}
Es sei $X$ eine Zufallsvariable und $t \in \R$. Wir setzen
\begin{itemize}
 \item $\{X=t\} := \{\omega \colon X(\omega) = t \} (= X^{-1}(t))$
 \item $\{X \geq t\} := \{\omega \colon X(\omega) \geq t \}$
\end{itemize}

\section{Definition}
Sind $X,Y$ Zufallsvariablen, so definiert man
\begin{itemize}
	\item $(X+Y)(\omega) = X(\omega) + Y (\omega)$
	\item $(X-Y)(\omega) = X(\omega) - Y (\omega)$
	\item $(X \cdot Y)(\omega) = X(\omega) \cdot Y (\omega)$
\end{itemize}
$\omega \in \Omega, \text{neue Zufallsvariablen } X+Y, X-Y, X \cdot Y$
\newline
analog für $a \in \R$
\begin{itemize}
	\item $a X(\omega) = a \cdot (X(\omega))$
	\item $\min(X,Y) = (X \wedge Y)(\omega):= \min \{X(\omega), Y(\omega)\} \ldots$
\end{itemize}

\section{Definition}
Sei $A \subset \Omega$. Die Funktion $1_A \colon \Omega \rightarrow \R$ ist definiert durch
$$1_A(\omega) = \begin{cases} 1 & \text{, falls } \omega \in A \\
							  0 & \text{, falls } \omega \notin A						
\end{cases}
$$
und heißt \textbf{Indikatorfunktion} von $A$.

\section{Bemerkungen (Rechenregeln für Indikatorfunktionen)}

\begin{itemize}
	\item $1_{\emptyset} \equiv 0$
	\item $1_{\Omega} \equiv 1$
	\item $(1_A)^2 = 1_A$
	\item $1_{A^c} = 1 - 1_A$
	\item $1_{A \cap B} = 1_A \cdot 1_B$
	\item $1_{A \cup B} = 1_A + 1_B - 1_{A \cap B}$
	\item $A \subset B \Leftrightarrow 1_A \leq 1_B$
	\item $1_{A \triangle B} = |1_A - 1_B|$
\end{itemize}

\section{Definition}
Seien $A_1, \ldots, A_n \subset \Omega$. Die Zufallsvariable 
$$X := \sum\limits_{j=1}^{n}{1_{A_j}}$$ heißt \textbf{Zählvariable} oder \textbf{Indikatorsumme}.

\paragraph{Bemerkung}
\begin{itemize}
	\item $\{X=0\} = \{\omega \colon X(\omega) = 0 \} = A_{1}^c \cap \ldots A_{n}^c$
	\item $\{X = n \} = A_1 \cap \ldots \cap A_n$
	\item $\{X = k\} = $ "genau k der Ereignisse $A_1, \ldots, A_n$ treten ein" = $\bigcup\limits_{T \subset \{1, \ldots, n\}, |T|=k}{\bigl (\bigcap\limits_{j \in T}{A_j} \cap \bigcap\limits_{j \notin T}{A_{j}^c} \bigr )}$ \newline
$(T \subset \{1, \ldots, n\}, |T| = \text{card } T = k)$
\end{itemize}

\chapter{Diskrete Wahrscheinlichkeitsräume}

\section{Motivation}
Zufallsexperiment mit Ausgängen in $\Omega$
\newline
n-malige, `unabhängige' Wiederholung
\newline
$\Rightarrow$ Ergebnis $(a_1, \ldots, a_n) \in \Omega^n$
\newline
$r_n(A):= \frac{1}{n} \sum\limits_{j=1}^{n}{1_A(a_j)}, A \subset \Omega$ relative Häufigkeit von $A$
\newline
$0 \leq r_n(A) \leq 1, r_n(\emptyset) = 0, r_n(\Omega) = 1$
\newline
$r_n(A \cup B) = r_n(A) + r_n(B), A \cap B = \emptyset$
\newline
empirisches Gesetz über Stabilisierung relativer Häufigkeiten:
\newline
$r_n(A) \underset{n \rightarrow \infty}{\leadsto} ? $

\section{Definition}
	Ein Paar $(\Omega, \mathbb{P})$ bestehend aus einer diskreten Menge $\Omega \neq \emptyset$ und einer Funktion $\mathbb{P} \colon \mathcal{P} \rightarrow \R$ heißt \textbf{diskreter Wahrscheinlichkeitsraum}, falls:
	\begin{itemize}
		\item (P1) $\mathbb{P}(A) \geq 0, A \subset \Omega$
		\item (P2) $\mathbb{P}(\Omega) = 1$
		\item (P3) $\mathbb{P}(\bigcup\limits_{j = 1}^{\infty}{A_j}) = \sum\limits_{j=1}^{\infty}{\mathbb{P}(A_j)}, A_i \cap A_j = \emptyset, i \neq j$
		\newline
		Diese Eigenschaft heißt $\sigma$-Additivität.
	\end{itemize}
	Man nennt $\mathbb{P}$ \textbf{Wahrscheinlichkeitsmaß (auf $\Omega$)} (oder Wahrscheinlichkeitsverteilung) und $\mathbb{P}(A)$ heißt \textbf{Wahrscheinlichkeit von A}.
	
\section{Folgerung}
\begin{itemize}
	\item a) $\Prim(\emptyset) = 0$
	\item b) $\Prim(\bigcup\limits_{j=1}^{n}{A_j}) = \sum \limits_{j=1}^{n}{\Prim(A_j)}, A_i \cap A_j = \emptyset, i \neq j$
	\item c) $0 \leq \Prim(A) \leq 1, A \subset \Omega$
	\item d) $\Prim(A \cup B) = \Prim(A) + \Prim(B) - \Prim(A \cap B), A,B \subset \Omega$
	\item e) $A \subset B \Rightarrow \Prim(A) \leq \Prim(B)$ (Monotonie)
	\item f) $\Prim(A^c) = 1 - \Prim(A)$ (Komplementärwahrscheinlichkeit)
	\item g) $\Prim(\bigcup\limits_{j=1}^{\infty}{A_j}) \leq \sum \limits_{j=1}^{\infty}{A_j}$ (Subadditivität)
	\item h) $A_n \subset A_{n+1}, n \in \N \Rightarrow \Prim(\bigcup\limits_{n=1}^{\infty}{A_n}) = \lim \limits_{n \rightarrow \infty}{\Prim(A_n)}$ (Stetigkeit von unten)
	\item i) $A_n \supset A_{n+1}, n \in \N \Rightarrow \Prim(\bigcap\limits_{n=1}^{\infty}{A_n}) = \lim \limits_{n \rightarrow \infty}{\Prim(A_n)}$ (Stetigkeit von oben)
\end{itemize}

\paragraph{Beweis}
$\bullet$ a): $A_j = \emptyset, j \in \N$ (P3) \footnote{$\Prim(\emptyset) = \Prim(\emptyset \cup \emptyset) = \Prim(\emptyset) + \Prim(\emptyset) = 2 \cdot \Prim(\emptyset)$} $\Prim(\emptyset) = 0$.
\newline
$\bullet$ b): $A_{n+1} = A_{n+2} = \ldots = \emptyset$ in P3!
\newline
$\bullet$ c) + f): Für $A \subset \Omega$ gilt nach b) (für n = 2): \newline
	$1 = \Prim(\Omega) = \Prim(A \cup A^c) \overset{(b)}{=} \Prim(A) + \Prim(A^c)$
\newline
$\bullet$ d): Nach b) gilt $\Prim(A) = \Prim(A \backslash B) + \Prim(A \cap B)$, $\Prim(B) = \Prim(B \backslash A) + \Prim(A \cap B)$ und somit $\Prim(A) + \Prim(B) - \Prim(A \cap B) = \Prim(A \backslash B) + \Prim (B \backslash A) + \Prim(A \cap B) \overset{(b)}{=} \Prim(A \cup B)$
\newline
$\bullet$ e): Wegen $B = A \cup (B \backslash A)$ folgt\footnote{(aus der Additivität)}
	$\Prim(B) = \Prim(A) + \Prim(B \backslash A) \geq \Prim(A)$
\newline
$\bullet$ g): $B_1 := A_1, B_2 := A_2 \backslash A_1, \ldots , B_n := A_n \backslash (\bigcup \limits_{j=1}^{n-1}{A_j}), n \geq 2$.
\newline
Dann gilt $B_n \subset A_n$ und $\bigcup\limits_{j=1}^{n}{B_j} = \bigcup\limits_{j=1}^{n}{A_j}$ sowie $B_i \cap B_j = \emptyset, i \neq j.$
\newline
Es folgt aus (P3):
\newline
$\Prim(\bigcup\limits_{j=1}^{\infty}{A_j}) \overset{!}{=} \Prim(\bigcup\limits_{j=1}^{n}{B_j}) \overset {(P3)} {=} \sum\limits_{j=1}^{n}{\Prim(B_j)} \overset {e)}{\leq} \sum\limits_{j=1}^{n}{\Prim(A_j)}$ ($\infty$ ist zugelassen)
\newline
$\bullet$ h) + i): Übungsaufgabe

\section{Satz}
Seien $A_1, \ldots, A_n \subset \Omega.$ Setze
$$S_k := \sum\limits_{1 \leq i_1 < \ldots < i_k \leq n}{\Prim(A_{i_1} \cap \ldots \cap A_{i_k})}$$
Dann gilt
\begin{itemize}
	\item a) $\Prim(\bigcup\limits_{j=1}^{n}{A_j}) = \sum\limits_{k=1}^{n}{(-1)^{k-1} S_k}$ `Siebformel'
	\item b) $\Prim(\bigcup\limits_{j=1}^{n}{A_j}) \leq \sum\limits_{k=1}^{2s+1}{(-1)^{k-1}S_k}, s = 0, \ldots, \lfloor \frac{n-1}{2} \rfloor$
	\newline
	$\Prim(\bigcup\limits_{j=1}^{n}{A_j}) \geq \sum\limits_{k=1}^{2s}{(-1)^{k-1}S_k}, s = 1, \ldots, \lfloor \frac{n}{2} \rfloor$
\end{itemize}

\paragraph{Beweisidee für Siebformel}
vollständige Induktion nach $n$:
\newline
$\text{\underline{n=2:} }\Prim(A_1 \cup A_2) \overset{(d)}{=} \Prim(A_1) + \Prim(A_2) - \Prim(A_1 \cap A_2) = S_1 - S_2$
\newline
$\text{\underline{n=3:} }\Prim(\underbrace{A_1 \cup A_2} \cup A_3) \overset {(d)} {=} \Prim(A_1 \cup A_2) + \Prim(A_3) - \Prim((A_1 \cup A_2) \cap A_3)$ \footnote{$(A_1 \cup A_2) \cap A_3 = (A_1 \cap A_3) \cup (A_2 \cap A_3)$}
\newline
  $\overset {(d)} {=} \Prim(A_1) + \Prim(A_2) - \Prim(A_1 \cap A_2) + \Prim(A_3) - \Prim(A_1 \cap A_3) - \Prim(A_2 \cap A_3) + \Prim(A_1 \cap A_2 \cap A_3) = S_1 - S_2 + S_3$
  
\section{Definition + Satz}
a) Sei $(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum. Dann heißt $p \colon \Omega \rightarrow \R$ definiert durch $p(\omega) := \Prim(\{\omega\})$ \textbf{Wahrscheinlichkeitsfunktion} (von $\Prim$).
\newline
Es gilt $\Prim(A) = \sum\limits_{\omega \in A}{p(\omega)}, A\subset \Omega$.
\newline
b) Sind $\Omega$ diskret und $p \colon \Omega \rightarrow \R$ eine Abbildung mit $p(\omega)\geq 0$ und $\sum\limits_{\omega \in \Omega}{p(\omega)} = 1$, so erhält man vermöge $\Prim(A):= \sum\limits_{\omega \in A}{p(\omega)}$ einen diskreten Wahrscheinlichkeitsraum.

\paragraph{Beweis}
$\bullet$ a) $\sigma$-Additivität ($A = \bigcup\limits_{\omega \in A}{\{\omega\}}$)
\newline
$\bullet$ b) $\sigma$-Additivität: Großer Umordnungssatz! (Analysis)

\section{Definition}
$|\Omega| =: n < \infty$. Definiere $\Prim(A)= \frac{|A|}{n}.$
Dann heißt $(\Omega, \Prim)$ (ein diskreter Wahrscheinlichkeitsraum!) \textbf{Laplace-Raum}. Man nennt $\Prim$ \textbf{Gleichverteilung} auf $\Omega$. 
\newline
(`homogene Münze', `Würfeln', \ldots)

\section{Definition}
Sei $\Omega \neq \emptyset$ beliebig!
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum $\Leftrightarrow \exists$ abzählbare Menge $\Omega_0 \subset \Omega$, $\exists p \colon \Omega \rightarrow [0, \infty)$ mit $p(\omega=0)$ für alle $\omega \notin \Omega_0$, und $\sum\limits_{\omega \in \Omega_0}{p(\omega)} = 1$, und $\Prim(A) = \sum \limits_{\omega \in A \cap \Omega_0}{p(\omega)}, A \subset \Omega$.

\paragraph{Wiederholung}
$(\Omega, \Prim)$ Wahrscheinlichkeitsraum
\newline
$p \colon \Omega \rightarrow [0,1], \sum\limits_{\omega \in \Omega}{p(\omega)} = 1$
\newline
$\Prim(A):=\sum\limits_{\omega \in A}{p(\omega)}, A \subset \Omega$
\newline
$p(\omega) := \Prim(\{\omega\})$
\newline
\hrule
\vspace{10pt}
$\Omega$ allgemeine Menge, $\Omega_0 \subset \Omega$ diskret
\newline
$p \colon \Omega \rightarrow [0,1], \sum\limits_{\omega \in \Omega_0}{p(\omega)}=1, p(\omega) = 0, \omega \notin \Omega_0$
\newline
$\Prim(A):= \sum\limits_{\omega \in A}{p(\omega)}:=\sum\limits_{\omega \in A \cap \Omega_0}{p(\omega)}$
\newline
$\Omega_0$= Träger von $\Prim$

\section{Definition}
\label{defVerteilung}
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum  mit Träger $\Omega_0$.
Es sei $X \colon \Omega \rightarrow \R$ eine Zufallsvariable.
Dann heißt die Funktion $\Prim^X \colon \Prim(\R) \rightarrow \R$ definiert durch 
$\Prim^X(B):= \Prim\left(X^{-1}(B)\right), B \subset \R$ \textbf{Verteilung um $X$}.

\section{Satz}
In der Situation von Definition \ref{defVerteilung} ist $(\R,\Prim^X)$ ein diskreter Wahrscheinlichkeitsraum mit Träger $B_0 := X(\Omega_0) = \{X(\omega) \colon \omega \in \Omega_0 \}$ 

\begin{proof}
Für $B \subset \R$.
\newline
$$\Prim^X(B) = \Prim(\{ \omega \colon X(\omega) \in B\})$$
$$\overset{!}{=} \Prim(\{\omega \colon X(\omega) \in B \cap B_0 \})$$
Definiert man für $t \in \R$
$$p_t = \Prim(\{\omega \colon X(\omega) = t\}) = \Prim(X = t)$$
so ergibt sich aus der $\sigma$-Additivität von $\Prim$
$$\Prim^X(B) = \sum\limits_{t \in B \cap B_0}{\Prim(\{\omega \colon X(\omega) = t \})} = \sum \limits_{t \in B \cap B_0}{p_t}$$
\end{proof}

\chapter{Kombinatorik}
$|A| = card(A)$ = Anzahl der Elemente einer endlichen Menge $A$

\section{Grundregeln}
$A_1, \ldots, A_k$ endliche Menge
\newline
(i) $A_i \cap A_j = \emptyset, i \neq j \Rightarrow \left|\bigcup\limits_{j=1}^{n}{A_j}\right| = \sum\limits_{j=1}^{n}{A_j}$
\newline
\label{grundregeln}
(ii) $|A_1 \times \ldots \times A_n| = \prod\limits_{j=1}^{k}{|A_j|}$

\section{Satz}
\label{urnen}
Es sollen k-Tupel $(a_1, \ldots, a_k)$ durch sukzessives Festlegen von $a_1,a_2,\ldots,a_k$ nach folgenden Regeln gebildet werden:
\begin{itemize}
	\item es gibt $j_1$ Möglichkeiten für die Wahl von $a_1$
	\item es gibt (dann) $j_2$ Möglichkeiten für die Wahl von $a_2$
	\item \ldots
	\item es gibt (dann) $j_k$ Möglichkeiten für die Wahl von $a_k$
\end{itemize}
Dann gibt es genau $j_1 \cdot \ldots \cdot j_k$ solcher Tupel.

\section{Beispiel (Urnenmodelle)}
Betrachte Urne mit $n$ durchnummerierten Kugeln.Es werden $k$ Kugeln nach folgenden Regeln gezogen: ($M:=\{1,\ldots,n\}$)
\newline
\begin{tabular}{|c|p{4cm}|p{4cm}|} \hline 
 \begin{tiny} \backslashbox{Zurücklegen\\ (Wiederholung)}{Beachtung der\\ Reihenfolge} \end{tiny} & ja & nein \\ \hline
ja & $k$-Permutationen aus $M$ mit Wiederholung, $Per_k^n$ & $k$-Kombinationen aus $M$ mit Wiederholung, $Kom_k^n$ \\ \hline
nein & $k$-Permutationen aus $M$ ohne Wiederholung, $Per_{k,\neq}^n$ & $k$-Kombinationen aus $M$ ohne Wiederholung, $Kom_{k,\neq}^n$ \\ \hline
\end{tabular}

\section{Definition} $M=\{1, \ldots,n\} (n \in \N)$
\begin{itemize}
	\item $Per_k^{n} := M^k$
	\item $Per_{k,\neq}^{n} := \{(a_1,\ldots,a_k) \in M^k \colon a_i \neq a_j \text{ für } i \neq j\}$
	\item $Kom_k^{n} := \{(a_1,\ldots,a_k= \in M^k \colon a_1 \leq a_2 \leq \ldots \leq a_k\}$
	\item $(Kom_{k,\neq}^{n} := \{(a_1,\ldots,a_k) \in M^k \colon a_1 < a_2 < \ldots < a_k\}$
\end{itemize}

\section{Satz}
\begin{itemize}
	\item (i) $|Per_k^n| = n^k$
	\item (ii) $|Per_{k,\neq}^n| := n^{\underline{k}} = n \cdot (n-1) \cdot \ldots \cdot (n-k+1)$
	\item (iii) $|Kom_k^n| = {{n+k-1} \choose {k}}$
	\item (iv) $|Kom_{k,\neq}^n| = {n \choose k}$
\end{itemize}

\begin{proof}
	(i): \ref{grundregeln}.(ii)
	\newline
	(ii) Satz \ref{urnen}
	\newline
	(iv) Betrachte Äquivalenzrelation $$(a_1,\ldots,a_k) \sim (b_1,\ldots,b_k) \Leftrightarrow \{a_1,\ldots,a_k\} = \{b_1,\ldots,b_k\}$$ auf $Per_{k,\neq}^n$. Jede Äquivalenzklasse hat $k!$ Elemente! Es folgt
		$$|Kom_{k,\neq}^n| \cdot k! = |Per_{k,\neq}^n| = n^{\underline{k}}$$
	(iii) Die Abbildung $g \colon Kom_k^n \rightarrow Kom_{k,\neq}^{n+k-1}$ definiert durch $$(a_1,\ldots,a_k) \mapsto (a_1,a_2+1,a_3+2,\ldots,a_k+k-1)$$ ist eine Bijektion! (Umkehrabbildung!) Es folgt(!)
		$$|Kom_k^n| = |Kom_{k,\neq}^{n+k-1}| = {{n+k-1} \choose k}$$
\end{proof}

\section{Beispiel (Geburtstagsproblem)}
Wie groß ist die Wahrscheinlichkeit, dass unter $k$ rein zufällig ausgewählten Personen mindestens zwei am selben Tag Geburtstag haben?

\paragraph{Antwort}
Betrachte $\Sigma = Per_k^n$ mit $n=365$, und der Laplace-Verteilung.
\newline
Es sei $A:=\left\{(a_1,\ldots,a_k) \in \Omega \colon \text{ es gibt } i,j \in \{1,\ldots,k\} \text{ mit } i \neq j, a_i = a_j \right \}$.
Es gilt
$$\Prim(A) = 1- \Prim(A^c)$$
$$= 1- \Prim(Per_{k,\neq}^n)$$
$$\overset{!}{=}1-\frac{|Per_{k,\neq}^n|}{card \Omega}$$
$$= 1- \frac{n \cdot (n-1) \cdot \ldots \cdot (n-k+1)}{n^k}$$
$$= 1- \frac{n}{n} \cdot \frac{(n-1)}{n} \cdot \ldots \cdot \frac{n-k+1}{n}$$
$$= 1- (1-\frac{1}{n}) \cdot \ldots \cdot (1- \frac{k-1}{n})$$
\underline{$k=23$:} $\Prim(A) \approx 0,507 > \frac{1}{2}$
\newline
$n={49 \choose 6}, k=4004, \Prim(A) = 0,5001 > \frac{1}{2}$

\section{Beispiel}
$n$ Personen bringen (zu einer Feier) je ein Geschenk mit. Geschenke werden "rein zufällig" verteilt. Mit welcher Wahrscheinlichkeit bekommt mindestens eine Person ihr eigenes Geschenk? 
\newline
$\leadsto$ Siebformel!

\section{Beispiel (Besetzungsmodelle)}
$k$ Teilchen sollen auf $n$ nummerierte Fächer verteilt werden. Analogie zu Urnenmodell: Nummer der Kugel $\hat{=}$ Nummer des Fachs, Nummer der Ziehung $\hat{=}$ Nummer des Teilchens

\begin{tabular}{|c|p{3cm}|p{3cm}|} \hline 
 \begin{tiny} \backslashbox{Unterscheidbare\\ Teilchen}{Mehrfachbesetzungen} \end{tiny} & ja & nein \\ \hline
ja & $Per_k^n$ \textcolor{blue}{Maxwell-Boltzmann} & $Kom_k^n$ \textcolor{blue}{Bose-Einstein-Statistik}\\ \hline
nein & $Per_{k,\neq}^n$ \textcolor{blue}{Fermi-Dirak-Statistik} & $Kom_{k,\neq}^n$ \\ \hline
\end{tabular}
\textcolor{blue}{Statistische Physik}

\chapter{Der Erwartungswert}
$p(\omega) = \Prim(\{\omega\})$, $(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum

\section{Definition}
\begin{itemize}
	\item Der Erwartungswert einer Zufallsvariablen $X \colon \Omega \rightarrow \R$ existiert (genauer: $X$ ist integrierbar bezüglich $\Prim$), falls
	\begin{equation}
		\label{ewert}
		\sum\limits_{\omega \in \Omega}{|X(\omega)| p(\omega)} < \infty
	\end{equation}
	In diesem Fall heißt 
		$$\E X = \E[X] := \sum\limits_{\omega \in \Omega}{X(\omega)p(\omega)}$$
		(Physik: $<X>=\E[X]$) \textbf{Erwartungswert von $X$}.
	\item Ist $X \geq 0$ eine Zufallsvariable, so heißt
		$$\E X := \sum\limits_{\omega \in \Omega}{X(\omega)p(\omega)} \in [0,\infty]$$
		ebenfalls Erwartungswert von $X$.
\end{itemize}

\section{Satz}
Sei $L^1 \equiv L^1(\Prim) := \{X \colon X \text{ erfüllt } \ref{ewert} \}$. Dann ist $L^1$ ein reeller Vektorraum. Genauer:
\begin{itemize}
	\item (i) $\E [X+Y] = \E X + \E Y, X,Y \in L^1$
	\item (ii) $\E [aX] = a \E X, X \in L^1, a \in \R$
	\item (iii) $\E 1_A = \Prim(A), A \subset \Omega$
	\item (iv) $X \leq Y \Rightarrow \E X \leq \E Y$
	\item (v) $|\E X| \leq \E |X|$
\end{itemize}

\begin{proof}
	(i) $|(X+Y)(\omega)| \leq |X(\omega)| + |Y(\omega)|$.
	\newline
	Also $X+Y \in L^1(\Prim)$ und
	$$\sum\limits_{\omega \in \Omega}{(X(\omega)+Y(\omega)) p(\omega)} \overset{!}{=} \sum\limits_{\omega \in \Omega}{X(\omega)p(\omega)} + \sum\limits_{\omega \in \Omega}{Y(\omega)p(\omega)}$$
	(ii) analog
	\newline
	(iii) $\E 1_A = \sum\limits_{\omega \in \Omega}{p(\omega)}=\Prim(A)$
	\newline
	(iv) + (v) Übungsaufgabe
\end{proof}

\section{Folgerung}
\label{folg}
Seien $A_1, \ldots, A_n \subset \Omega$ und $X := \sum\limits_{j=1}^{n}{1_{A_j}}$. Dann gilt $\E X = \sum\limits_{j=1}^{n}{\Prim(A_j)}$. \newline
(Gilt auch für $\infty$ viele Ereignisse.)

\section{Satz (Transformationsformel)}
Seien $X \colon \Omega \Rightarrow \R$ und $g \colon \R \rightarrow \R$. Definiere $g(X) \colon \Omega \rightarrow \R$ durch $$g(X)(\omega) = g(X(\omega)).$$ Dann ist $g(X) \in L^1(\Prim)$ genau dann, wenn
$$\sum\limits_{x \colon \Prim(X=x)>0}{|g(x)| \Prim(X=x)} < \infty$$\footnote{$\Prim(X=x) = \Prim(\{\omega \in \Omega \colon X(\omega)=x\})$}

In diesem Fall gilt
$$\E g(x) = \sum\limits_{x \colon \Prim(X=x)>0}{g(x) \Prim(X=x)}$$

\begin{proof}
	Es gilt
	$$\sum\limits_{\omega \in \Omega}{|g\left(X(\omega)\right)|p(\omega)} = \sum\limits_{x \in \R \colon \Prim(X=x)>0}{|g(x)|} \sum\limits_{\omega \in \Omega \colon X(\omega) = x}{p(\omega)}$$
	\hrule
	$\Omega = \bigcup\limits_{x \in \R \colon X(\omega) = x, \Prim(X=x)>0}{\{\omega \colon X(\omega) = x\}} \cup \Omega^\prime, \Prim(\Omega^\prime) = 0$
	\hrule
	$$= \sum{|g\left(X(\omega)\right)|p(\omega)} = \sum\limits_{\omega \notin \Omega^\prime}{|g\left(X(\omega)\right)|p(\omega)}$$
	$$= \sum\limits_{x \in \R \colon \Prim(X=x)>0}{\sum\limits_{\omega \in \{\omega \in \Omega \colon X(\omega) = x \}}{|g(X(\omega))|p(\omega)}}$$
	$$= \sum\limits_{x \cdots}{|g(x)| \Prim(X=x)}$$
	Ist das endlich, so gilt die Rechnung auch ohne Betragsstriche!
	\newline
	Insbesondere gilt
	$$\E X = \sum\limits_{x \in \R}{x \Prim(X=x)} \left(g(x)\equiv x\right)$$
\end{proof}

\section{Beispiele}
\begin{itemize}
	\item Würfelwurf, $X$=Augenzahl, $\Prim(X=j)=\frac{1}{6}$.
		\newline Also $$\E X = \sum\limits_{j=1}^{6}{j \cdot \Prim(X=j)}=\frac{6\cdot 7}{2} \cdot \frac{1}{6} = \frac{7}{2} = 3,5$$
	\item Zweifacher Würfelwurf, $X$=Maximum der Augenzahlen ($\Omega=\{1,\ldots,6\}^2, \Prim = \text{Gleichverteilung}$)
		 $$\Prim(X=1)=\frac{1}{36}$$
		 $$\Prim(X=2)=p((1,2)) + p((2,1)) + p((2,2)) = \frac{3}{36}$$
		 \underline{Allgemein:} $\Prim(X=j)=\frac{2j-1}{36}, j =1,\ldots,6$
		 \newline
		 Es folgt
		 $$\E X = \sum\limits_{j=1}^{6}{j \cdot \frac{2j-1}{36}} \overset{?}{\approx} 4,47$$
\end{itemize}

\chapter{Die hypergeometrische Verteilung und die Binomialverteilung}
Urne mit Kugeln $\underbrace{1,2,\ldots,r}_{rot}, \underbrace{r+1,\ldots,r+s}_{schwarz}$
\newline $r,s \in \N_0, r+s > 0$.

\section{Definition}
\begin{itemize}
	\item $n$ mal Ziehen ohne Zurücklegen
	\item $a_j :=$ Nummer der $j$-ten gezogenen Kugel
	\item $\Omega = Per_{n,\neq}^{r+s}$
	\item $\Prim =$ Gleichverteilung ("unabhängiges", "rein zufälliges" Ziehen)
	\item $A_j := \{(a_1,\ldots,a_n) \in \Omega \colon a_j \leq r\} \hat{=} \{\text{j-te gezogene Kugel ist rot}\}$
	\item $X:= \sum\limits_{j=1}^{n}{1_{A_j}}$= Anzahl der gezogenen roten Kugeln
\end{itemize}
$\Prim^X$ (die Verteilung von $X$) heißt \textbf{hypergeometrische Verteilung} mit Parametern $r,s,n$, kurz:
$$X \sim Hyp(n,r,s), n \leq r+s$$
$$\Prim^X = Hyp(n,r,s)$$

\section{Satz}
Es gilt
\begin{itemize}
	\item (i) $\E X = n \cdot \frac{r}{r+s}$
	\item (ii) $\Prim(X=k) = \frac{{r \choose s}{s \choose {n-k}}}{{{r+s} \choose {n}}}, k = 0, \ldots, r \wedge n$
\end{itemize}

\begin{proof}
	(i) Es gilt (Symmetrieargument!) $|A_j| = r \cdot (r+s-1)^{\underline{n-1}}$
	\newline
	$|\Omega| = (r+s)^{\underline{n}}$
	$\Rightarrow \Prim(A_j) = \frac{|A_j|}{|\Omega|} = \frac{r}{r+s}$
	\newline
	Aus \ref{folg} folgt $\E X = n \cdot \frac{r}{r+s}$
	\newline
	(ii) $|\{X=k\}| \overset{!}{=} {n \choose k} r^{\underline{k}} s^{\underline{n-k}}$
	\newline
	$\Rightarrow \Prim(X=k) = \frac{{n \choose k} r^{\underline{k}} s^{\underline{n-k}}}{(r+s)^{\underline{n}}} = \frac{{r \choose k}{s \choose {n-k}}}{{{r+s}\choose n}}$
\end{proof}

\section{Motivation}
$X$ Zufallsvariable, $\sum\limits_{k=1}^r{\Prim(X=x_n)} = 1$
\newline
$X_1, X_2, \ldots, X_n$ "unabhängige" Wiederholungen von $X$ (= Ergebnis eines zufälligen Versuchs)
\newline
$\bar{X}:=\frac{1}{n}(X_1 + \ldots + X_n)$ Zufallsvariable!
\newline
Mit $h_j := card\{i \in \{1,\ldots,n\} \colon X_i = x_j \}$ gilt $\bar{X} \overset{!}{=} \frac{1}{n}(h_1x_1 + h_2x_2 + \ldots + h_nx_n)$
\newline
empirisches Gesetz über Stabilität relativer Häufigkeiten
\newline
$\underset{n \rightarrow \infty}{\rightarrow} \Prim(X=x_1)x_1 + \ldots + \Prim(X=x_r)x_r \overset{!}{=} \E X$
\newline
$X \sim Hyp(n,r,s) = \Prim^X, n \leq r +s$
\newline
$\Prim(X=k)=\frac{
			{{r \choose k}{s \choose {n-k}}}}{{{r+s}\choose n}}, k = 0, \ldots, n$
\newline
Wegen ${m \choose l}:= 0$ für $m<l$ gilt:
$\Prim(X=k)=0$ für $k < r$ und für $n-k>s (k < n-s)$

\section{Definition}
\textbf{Binomialverteilung}:
\begin{itemize}
	\item n maliges Ziehen aus einer Urne mit $r+s$ Kugeln mit Zurücklegen
	\item $\Omega = Per_n^{r+s} = \{(a_1,\ldots,a_n) \colon 1 \leq a_i \leq r+s, i = 1, \ldots, n\}$
	\item $\Prim$ Gleichverteilung
\end{itemize}
$X:= \sum\limits_{j=1}^n {1_{A_j}}, A_j := \{(a_1,\ldots,a_n) \in \Omega \colon a_j \leq r\}$
\newline
$\Prim^X$ heißt Binomialverteilung mit Parametern n und $p:=\frac{r}{r+s}$. Man schreibt auch $Bin(n,p):=\Prim^X$.

\section{Satz}
Es gilt
\begin{enumerate}
	\item $\E X = np$
	\item $\Prim(X=k)= {n \choose k} p^k (1-p)^{n-k}, 0 \leq k \leq n$
\end{enumerate}

\begin{proof}
\begin{enumerate}
	\item $|A_j|=r \cdot (r+s)^{n-1}$
		\newline
		$|\Omega| = (r+s)^n \leadsto \Prim(A_j) = \frac{|A_j|}{|\Omega|}= \frac{r}{r+s}=p$
		\newline
		Folgerung 5.3 $\leadsto \E X = np$.
	\item $card\{X=k\} = {n \choose k} r^k s^{n-k}$
		\newline
		$\leadsto \Prim(X=k)=\frac{{n \choose k} r^k s^{n-k}}{(r+s)^k (r+s)^{n-k}}$
\end{enumerate}
\end{proof}

\paragraph{Bemerkung}
$Bin(n,p)$ ist für jedes $p \in [0,1]$ definiert.

\chapter{Mehrstufige Experimente}
\section{Beispiel}
Urne mit einer roten und drei schwarzen Kugeln
\newline
\underline{1. Experiment} Kugel ziehen, Farbe notieren, Kugel und eine weitere Kugel derselben Farbe zurücklegen
\newline
\underline{2. Experiment} Erneut Kugel ziehen
\newline
Modell: $\Omega := \{0,1\} \times \{0,1\}, \quad (0 \hat{=} s, 1 \hat{=} r)$
\paragraph{Konstruktion von $\Prim$}
$p(\omega):= \Prim(\{\omega\})$

$\left.
\begin{array}{cc} % für mehrzeiligen Text nötig
p(1,1) := \frac{1}{4} \cdot \frac{2}{5} = \frac{2}{20} = \frac{1}{10}
 \\ p(1,0) := \frac{1}{4} \cdot \frac{3}{5} = \frac{3}{20} 
 \\ p(0,1) := \frac{3}{4} \cdot \frac{1}{5} = \frac{3}{20} 
 \\ p(0,0) := \frac{3}{4} \cdot \frac{4}{5} = \frac{12}{20}
\end{array}
\right\}
\text{1. Pfadregel}
$
$$\sum\limits_{\omega \in \Omega}{p(\omega)} = 1.$$
Betrachte $B:= \{(1,1),(0,1)\}$.
Dann gilt
$$\Prim(B) = p(1,1) + p(0,1) = (\text{2. Pfadregel})$$
$$= \frac{2}{20} + \frac{3}{20} = \frac{1}{4} \overset{!}{=} \Prim(\text{erste Kugel ist rot})$$
(TODO: Bild(Baumdiagramm))

\section{Definition}
\paragraph{Mehrstufige Experimente}
$\Omega = \Omega_1 \times \ldots \times \Omega_n$ ($\Omega_j \hat{=}$ Grundraum für $j$-tes Teilexperiment)
\newline
$\omega = (a_1, \ldots, a_n) \in \Omega$
\newline
Problem: Definiere $p(\omega) = \Prim(\{\omega\})$
\begin{enumerate}
	\item Startverteilung $p_1 \colon \Omega_1 \rightarrow [0,1] \quad \sum\limits_{\omega \in \Omega_1}{p_1(\omega)} = 1$
	\item Übergangswahrscheinlichkeiten $p_2(a_2 | a_1) \geq 0 \quad \sum\limits_{a_2 \in \Omega_2}{p_2(a_2|a_1)} \overset{!}{=} 1$ 
	
	$(p_2(a_2|a_1) \hat{=}$ Wahrscheinlichkeit, dass 2. Versuch das Ergebnis $a_2$ liefert unter der Bedingung, dass 1. Versuch Ergebnis $a_1$ geliefert hat.)
	\newline
	$p_3(a_3|a_1,a_2) \geq 0 \quad \sum\limits_{a_3 \in \Omega_3}{p_3(a_3|a_1,a_2)} = 1$
	\newline	
	...
	\newline
	$p_n(a_n | a_1, \ldots, a_{n-1}) \geq 0 \quad \sum\limits_{a_n \in \Omega_n}{p_n(a_n|a_1, \ldots, a_{n-1})} = 1$
\end{enumerate}
Setze für $\omega = (a_1, \ldots, a_n) \in \Omega$
$$p(\omega) := p_1(a_1) \cdot p_2(a_2|a_1) \cdot p_3(a_3|a_1,a_2) \cdot \ldots \cdot p_n(a_n|a_1,\ldots,a_{n-1}) \quad \text{1. Pfadregel}$$
Schließlich sei
$$\Prim(A) := \sum\limits_{\omega \in A}{p(\omega)}, \quad A \subset \Omega \qquad \text{Produkt von Übergangswahrscheinlichkeiten}$$

\section{Satz}
$(\Omega, \Prim)$ ist diskreter Wahrscheinlichkeitsraum.

\begin{proof}
	zu zeigen: $\sum\limits_{\omega \in \Omega}{p(\omega)} = 1$
	\newline
	Induktion (oder direkt)! Zum Beispiel gilt für $n=2$
	$$\sum\limits_{\omega \in \Omega}{p(\omega)} = \sum\limits_{(a_1,a_2) \in \Omega_1 \times \Omega_2}{p_1(a_1) p_2(a_2|a_1)} = \sum\limits_{a_1 \in \Omega_1}{\sum\limits_{a_2 \in \Omega_2}{p_1(a_1) p_2(a_2|a_1)}}$$
	$$\sum\limits_{a_1 \in \Omega_1}{p_1(a_1) \cdot 1} = 1.$$
\end{proof}

\section{Beispiel}
\paragraph{Unabhängige Experimente}
$(\Omega_j,\Prim_j), j=1,\ldots,n$, diskrete Wahrscheinlichkeitsräume, $p_i(a_i)=\Prim_i(\{a_i\})$
\newline
Idee: "Unabhängiges" Durchführen der zugehörigen Experimente
$$\Omega := \Omega_1 \times \ldots \times \Omega_n, p(\omega):= p_1(a_1) \cdot \ldots \cdot p_n(a_n), \omega=(a_1, \ldots, a_n) \in \Omega$$
$\big(p_2(a_2|a_1) = p_2(a_2), \ldots, p_n(a_n|a_1, \ldots, a_{n-1})=p_n(a_n)\big)$

$$\Prim(A) = \sum\limits_{\omega \in A}{p(\omega)}$$
Man nennt $\Prim$ das \textbf{Produkt} von $\Prim_1, \ldots, \Prim_n$ und schreibt 
	$$\Prim := \bigotimes\limits_{i=1}^n{\Prim_i}.$$
	z.B. kann $\Omega = \Omega_1 \times \Omega_2, \quad \Omega_1 = \Omega_2 = \{1,\ldots,6\}$
	\newline
	$p_1(a_1)=p_2(a_2)=\frac{1}{6}$
	\newline
	Dann ist 
		$$p(a_1,a_2)=\frac{1}{6} \cdot \frac{1}{6} = \frac{1}{36}$$
		und $\Prim$ ist die Laplace-Verteilung auf $\Omega$.
		
\chapter{Bedingte Wahrscheinlichkeiten}
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum.
\section{Definition}
Sei $B \subset \Omega$ mit $\Prim(B)>0$. Dann heißt
$$\Prim(A|B):=\frac{\Prim(A \cap B)}{\Prim(B)}$$
\textbf{bedingte Wahrscheinlichkeit} von $A \subset \Omega$ unter der Bedingung $B$.
\newline
Alternativ: $P_B(A):=\Prim(A|B)$

\section{Satz}
$P_B$ ist ein Wahrscheinlichkeitsmaß auf $\Omega$.
Dabei ist $P_B(A)=1$ falls $B \subset A$ und $P_B(A) = 0$ falls $A \cap B = \emptyset$.
Es gilt:
$$p_B(\omega):= \begin{cases}
		\frac{p(\omega)}{\Prim(B)} & \text{, falls } \omega \in B \\
			0 & \text{, sonst}
	\end{cases} \qquad \text{ mit } p_B(\omega):=\Prim_B(\{\omega\})$$
	Beweis ist klar! ($\sum\limits_{\omega \in \Omega}{p_B(\omega)} = \frac{1}{\Prim(B)} \sum \limits_{\omega \in B}{p(\omega)} = \frac{\Prim(B)}{\Prim(B)}=1$.)
	
\paragraph{Motivation}
Für $A \subset B$
$$\frac{h_n(A)}{h_n(B)}= \frac{\frac{1}{n}h_n(A)}{\frac{1}{n}h_n(B)} \leadsto \frac{\Prim(A)}{\Prim(B)}.$$

\section{Bemerkung (Zusammenhang zu Übergangswahrscheinlichkeiten)}
$\Omega = \Omega_1 \times \Omega_2, \quad p(\omega) = p_1(a_1)p_2(a_2|a_1), \quad \omega = (a_1,a_2)$
\newline
Für $a_1 \in \Omega_1$ sei
$$B := \{a_1\} \times \Omega_2.$$
Für $a_2 \in \Omega_2$ sei
$$A := \Omega_1 \times \{a_2\}.$$
Es gilt $A \cap B = \{(a_1,a_2)\}$,
$$\Prim(A \cap B) = \sum\limits_{\omega \in A\cap B}{p(\omega)} = \sum\limits_{(b_1,b_2) \in A \cap B}{p_1(b_1) p_2(b_2|b_1)} = p_1(a_1) p_2(a_2|a_1),$$
$$\Prim(B) = \sum\limits_{b_2 \in \Omega_2}{p(a_1|b_2)} = \sum\limits_{b_2 \in \Omega_2}{p_1(a_1) p_2(b_2|a_1)} = p_1(a_1)$$
Es folgt
$$\Prim(A|B) = \frac{\Prim(A \cap B)}{\Prim(B)} \overset{p_1(a_1)>0}{=} p_2(a_2|a_1)$$

\section{Satz (Multiplikationsformel)}
Seien $A_1, \ldots, A_n \subset \Omega$ mit $\Prim(A_1 \cap \ldots \cap A_{n-1}) > 0$.
Dann gilt
$$\Prim(A_1 \cap \ldots \cap A_n) = \Prim(A_1) \Prim(A_2|A_1) \Prim(A_3|A_1 \cap A_2) \cdot \ldots \cdot \Prim(A_n|A_1 \cap \ldots \cap A_{n-1})$$

\begin{proof}
	Für $n=2$:
		$$\Prim(A_1 \cap A_2) = \Prim(A_1) \cdot \Prim(A_2|A_1)$$
	\underline{Allgemein:} Ausschreiben der Definitionen + kürzen
	\newline
	$n=3$: rechte Seite: $\Prim(A_1) \cdot \frac{\Prim(A_1 \cap A_2)}{\Prim(A_1)} = \frac{\Prim(A_1 \cap A_2 \cap A_3)}{\Prim(A_1 \cap A_2)} = \Prim(A_1 \cap A_2 \cap A_3)$
\end{proof}

\section{Satz}
Sei $A_1, A_2, \ldots$ Zerlegung von $\Omega (\bigcup{A_i} = \Omega, A_i \cap A_j = \emptyset, i \neq j)$.
\newline
Sei $B \subset \Omega$. Dann gilt
\begin{enumerate}
	\item $\Prim(B) = \sum\limits_{j=1}^{\infty}{\Prim(A_j)\Prim(B|A_j)}$ Formel der totalen Wahrscheinlichkeit
	\item \footnote{Formel von Bayes} Für $\Prim(B)>0$, so gilt $$\Prim(A_k|B) = \frac{\Prim(A_k)\Prim(B|A_k)}{\sum\limits_{j=1}^\infty{\Prim(A_j)\Prim(B|A_j)}},\quad k=1,2,\ldots$$
\end{enumerate}
(Man vereinbart $\Prim(B|A_j)\Prim(A_j):= 0$, falls $\Prim(A_j)=0$)

\begin{proof}
	\begin{enumerate}
		\item $B = B \cap \Omega = \bigcup\limits_{j=1}^\infty{\underbrace{B \cap A_j}_{\text{paarweise disjunkt}}}$ 
		Aus der $\sigma$-Additivität von $\Prim$ folgt
		$$\Prim(B)=\sum\limits_{j=1}^\infty{\Prim(B \cap A_j)} = \sum\limits_{j=1}^\infty{\Prim(B|A_j)\Prim(A_j)}$$
		\item rechte Seite der Behauptung: $\frac{\Prim(B \cap A_k)}{\Prim(B)} \overset{!}{=} \Prim(A_k|B)$
	\end{enumerate}
\end{proof}

\section{Beispiel}
Eine Krankheit komme bei $4 \%$ der Bevölkerung vor\footnote{Die Mediziner sprechen von "Prävalenz".}. Ein Test spreche bei $90 \%$ der Kranken an und bei $20 \%$ der Gesunden!
\paragraph{Modell}
\begin{itemize}
	\item $\Omega :$ Menge der Personen in Deutschland
	\item $K \subset \Omega :$ Menge der kranken Personen
	\item $A \subset \Omega :$ Menge der (hypothetisch) positiv getesteten Personen
	\item $\Prim$ = Gleichverteilung auf $\Omega$
\end{itemize}

Dann 
$$\Prim(K|A)=\text{ Wahrscheinlichkeit, dass eine positiv getestete Person krank ist}$$
$$\overset{Bayes}{=} \frac{\Prim(K)\Prim(A|K)}{\Prim(K)\Prim(A|K) + \Prim(K^c)\Prim(A|K^c)} \quad (K = A_j, K^c = A_k)$$
$$= \frac{0,04 \cdot 0,9}{0,04 \cdot 0,9 + 0,96 \cdot 0,2} = \frac{0,036}{0,036 + 0,192} = \frac{0,036}{0,228} = 0,158$$

\section{Beispiel (Ziegenproblem)} Ausgelassen.

\section{Beispiel (Simpson-Paradoxon)}
Zulassung von Studenten in Berkeley (1973)
\begin{itemize}
	\item Zulassungsrate Männer: $44 \%$
	\item Zulassungsrate Frauen: $35 \%$
\end{itemize}
\underline{Aber:} Zulassungsraten der Männer in den einzelnen Fächern kleiner als die der Frauen

\paragraph{Erklärung}
\begin{itemize}
	\item $A \hat{=}$ Zulassung \footnote{Ereignis, dass rein zufällig ausgewählter Bewerber erfolgreich ist mit seiner Bewerbung.}
	\item $B \hat{=}$ Frau \footnote{Ereignis, dass zufällig ausgewählte weibliche Bewerberin erfolgreich ist.}
	\item $K_j \hat{=}$ Bewerbung für Fach $j$
\end{itemize}

Dann kann gelten
$$\Prim(A|B) < \Prim(A | B^c)$$
aber
$$\Prim(A|B \cap K_j) > \Prim(A|B^c \cap K_j), \quad j=1,2,\ldots$$
Denn:
$$\Prim(A|B)= \frac{\Prim(A\cap B)}{\Prim(B)} = \sum\limits_{j}{\frac{\Prim(A \cap B\cap K_j)}{\Prim(B)} \frac{\Prim(B \cap K_j)}{\Prim(B \cap K_j)}}$$
$$ = \sum\limits_{j}{\underbrace{\Prim(K_j|B)}_{\text{Bewerbungsrate der Frauen im j-ten Fach}}\underbrace{\Prim(A|B \cap K_j)}_{\text{siehe oben}}}$$
analog
$$\Prim(A|B^c) = \sum{\Prim(K_j|B^c) \Prim(A|B^c \cap K_j)}$$

Die absolute Erfolgsquote ist eine gewichtete Summe der relativen Erfolgsquoten.

\chapter{Stochastische Unabhängigkeit}
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum.

\section{Definition}
$A_1, \ldots, A_n \subset \Omega$ heißen \textbf{stochastisch unabhängig}, falls

$$\Prim(\bigcap\limits_{j \in T}{A_j}) = \prod\limits_{j \in T}{\Prim(A_j)}, \quad T \subseteq \{1, \ldots, n\}, |T| \geq 2.$$

($2^n - n - 1$ Gleichungen.)

\section{Bemerkung}
\begin{enumerate}
	\item $A,B$ stochastisch unabhängig \newline
	$\Leftrightarrow \Prim(A \cap B) = \Prim(A)\Prim(B)$ \newline
	$\overset{\Prim(B)>0}{\Leftrightarrow} \Prim(A|B) = \Prim(A)$ (Interpretation!)
	\footnote{Wenn die Kenntnis des Eintretens von B keinerlei Rückschlüsse auf das Eintreten von A zulässt.} \newline
	$\Leftrightarrow \Prim(B|A) \overset{\Prim(B)>0}{=} \Prim(B)$
	
	\item $\Prim(B) = 0 \leadsto$ $A$ und $B$ sind stochastisch unabhängig \newline
		$\Prim(B) = 1 \leadsto$ $A$ und $B$ sind stochastisch unabhängig
		
	\item $A,B,C$ unabhängig $\Leftrightarrow$
		
		$\left.
\begin{array}{cc} % für mehrzeiligen Text nötig
\Prim(A \cap B) = \Prim(A)\Prim(B)
 \\ \Prim(A \cap C) = \Prim(A)\Prim(C) 
 \\ \Prim(B \cap C) = \Prim(B)\Prim(C) 
\end{array}
\right\}
\text{paarweise stochastische Unabhängigkeit}
$
		
		$$\Prim(A \cap B \cap C) = \Prim(A) \Prim(B) \Prim(C)$$
\end{enumerate}


\end{document}
