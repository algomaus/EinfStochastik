\documentclass[a4paper,11pt,notitlepage]{report}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{bibgerm}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{color}
\usepackage{enumerate}
\usepackage{tabularx}
\usepackage{subfig}
\usepackage{fancyhdr}
\usepackage[pdftex,pdfpagelabels,colorlinks,backref,pagebackref]{hyperref}
\usepackage{tikz} % SELBST HINZUGEFÜGT
\usepackage{slashbox}
% == Set the heading style ===================================================
\setlength{\headheight}{14pt}
\pagestyle{fancyplain}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\rightmark}}
\rhead[\fancyplain{}{\leftmark}]{\fancyplain{}{\thepage}}
\cfoot{}
\renewcommand{\headrulewidth}{0.4pt}
% ============================================================================

% == Set correct values for fitting floats ===================================
\tolerance=2000
\emergencystretch=10pt

\setcounter{topnumber}{3}
\setcounter{totalnumber}{5}
\setcounter{bottomnumber}{2}

% To make those darn floats fit where they should
\setcounter{totalnumber}{9}
\setcounter{topnumber}{9}
\setcounter{bottomnumber}{9}
\renewcommand{\textfraction}{0.00}
\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
% ============================================================================

% == German definitions for theorems etc. ==================================== 
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Satz}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{corollary}{Korollar}[chapter]
\newtheorem{observation}{Beobachtung}[chapter]
\newtheorem{fact}{Fakt}[chapter]
\newtheorem{remark}{Bemerkung}[chapter]
\newtheorem{example}{Beispiel}[chapter]
% ============================================================================

% == Abkürzungen für die reellen, natürlichen, ganzen,... Zahlen =============
\newcommand{\R}{{\ensuremath{\mathbb{R}}}}
\newcommand{\N}{{\ensuremath{\mathbb{N}}}}
\newcommand{\Z}{{\ensuremath{\mathbb{Z}}}}
\newcommand{\C}{{\ensuremath{\mathbb{C}}}}
\newcommand{\Q}{{\ensuremath{\mathbb{Q}}}}
\newcommand{\F}{{\ensuremath{\mathbb{F}}}}
\newcommand{\Prim}{{\ensuremath{\mathbb{P}}}}
\newcommand{\E}{{\ensuremath{\mathbb{E}}}}
% ============================================================================

% == Makros für Autorenname und -adresse =====================================
\newcommand{\myaddress}[6]{%
  \parbox{\textwidth}{\textbf{\large #1}\\
    #2\\ #3\\ #4\\ 
    \ifthenelse{\equal{#5}{}}{}{Email: \href{mailto:#5}{\texttt{#5}}\\}
    \ifthenelse{\equal{#6}{}}{}{WWW: \href{#6}{\path|#6|}\\}
  } 
}

\newcommand{\myauthor}[1]{%
  \addtocontents{toc}{\protect\hspace{3.35ex}%
  \textsl{#1}\par}\vspace{-4ex}\quad\hfill\textsl{\Large #1}\vspace{8ex}}

\newcommand{\myname}[1]{\Large #1}

\title{\textbf{{Einführung in die Stochastik - Mitschrieb} \\[5ex] 
    {\Large Vorlesung im Wintersemester 2011/2012\\[5ex]}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tragen Sie in der folg. Zeile Ihren Namen ein: %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{\myname{Sarah Lutteropp}}

\begin{document}
\shorthandoff{"}
\maketitle
\setcounter{tocdepth}{1}
\tableofcontents

\section*{Vorwort}
Dies ist ein Mitschrieb der Vorlesung “Einführung in die Stochastik” vom Wintersemester 2011/2012 am Karlsruher Institut für Technologie, die von Herrn Prof. Dr. Günther Last gehalten wird.

\chapter{Deskriptive Statistik}
\section{Der Grundraum}
$\emptyset \neq \Omega$ = Grundraum (Grundgesamtheit, Merkmalsraum, Stichprobenraum)
Annahme: $\Omega$ ist diskret(endlich oder abzählbar unendlich) (Häufig $\Omega \subseteq \R$)

\section{Absolute und relative Häufigkeit}
$x_1, \ldots, x_n \in \Omega$ ("Daten") \newline
$h(\omega) = \text{card}\left\{j\in\{1, \ldots, n\} \colon x_j = \omega\right\}, \omega \in \Omega$, absolute Häufigkeit von $\omega$

\paragraph{Bemerkung}
$\sum\limits_{\omega \in \Omega}{h(\omega)} = n$

\paragraph{Definition}
$\frac{1}{n} h(\omega)$ = relative Häufigkeit von $\omega$ \newline
$h(A)=\text{card}\left\{j\in\{1,\ldots,n\}\colon x_j \in A\right\}, A \subset \Omega$ = absolute Häufigkeit von A, $\frac{1}{n} h(A)$ = relative Häufigkeit von A

\section{Histogramm}
$x_1, \ldots, x_n \in \R, b_1 < b_2 < \ldots < b_s$ mit $b_1 \leq \min\limits_{1 \leq i \leq n}{x_i}, b_s > \max\limits_{1 \leq i \leq n}{x_i}$
\newline
TODO: BILD
\newline
$d_j(b_{j+1}-b_j)=h([b_j,b_{j+1})) = \text{card} \left\{i\in\{1,\ldots,n\}\colon b_j \leq x_i < b_{j+1}\right\}$

\section{Lagemaße}
\paragraph{Definition}
Ein \textbf{Lagemaß} ist eine Abbildung $l \colon \R^n \rightarrow \R$ mit $$l(x_1+a,\ldots,x_n+a) = l(x_1,\ldots,x_n)+a$$ "Verschiebungskovarianz".
$x_1,\ldots,x_n,a \in \R$

\subsection{Arithmetisches Mittel}
$x_1,\ldots,x_n \in \R, \bar{x} := \frac{1}{n} \sum\limits_{j=1}^{n}{x_j}$ "Schwerpunkt der Daten"

\paragraph{Fakt}
$\sum\limits_{j=1}^{n}{(x_i - t)^2} \overset{t}{\rightarrow} \text{Min}$
\newline
Lösung: $t = \bar{x}$
\newline
"Prinzip der kleinsten Quadrate"

\paragraph{Beweis}
$\frac{1}{n} \sum\limits_{j=1}^{n}{(x_j - t)^2} = t^2 - 2\bar{x}t + \frac{1}{n} \sum\limits_{j=1}^{n}{x_j^2} = (t - \bar{x})^2 + \frac{1}{n} \sum\limits_{j=1}^{n}{x_j^2 - (\bar{x})^2}$

\subsection{Median, Quantile}
$x_1,\ldots,x_n \in \R \Rightarrow x_{(1)} \leq x_{(2)} \leq \ldots \leq x_{(n)}$ geordnete Stichprobe

\paragraph{Definition}

$$x_{1/2}:= \begin{cases}
	x_{(\frac{n+1}{2})} & \text{, falls } n \text{ ungerade} \\
	\frac{1}{2}(x_{(\frac{n}{2})} + x_{(\frac{n}{2}+1)}) & \text{, falls } n \text{ gerade}
\end{cases}
$$ 
heißt \textbf{Median} von $x_1,\ldots,x_n$.

\paragraph{Fakt}
$\sum\limits_{j=1}^{n}{|x_j - x_{1/2}|} = \min\limits_{t}{\sum\limits_{j=1}^{n}{|x_j - t|}}$ Übungsaufgabe

\paragraph{Bemerkung}
Der Median ist "robust" gegenüber "Ausreißern".
Ist etwa $x_1 = \ldots = x_9 = 1$ und $x_{10} = 1000 (n=10)$, so gilt $\bar{x} = 100,9  , x_{1/2} = 1$

\paragraph{Definition}
Für 0 < p < 1 heißt
$$
x_p := \begin{cases}
	x_{(\lfloor n \cdot p + 1 \rfloor )} & \text{, falls } n \cdot p \notin \N \\
	\frac{1}{2}(x_{(n \cdot p)} + x_{(n \cdot p + 1)}) & \text{, falls } n \cdot p \in \N
\end{cases}
$$
\textbf{p-Quantil} von $x_1, \ldots, x_n$.

\paragraph{Interpretation}
Mindestens $p \cdot 100 \%$ der Daten liegen links von $x_p$ und mindestens $(1-p) \cdot 100 \%$ liegen rechts von $x_p$. \newline
$x_{1/4}=$ unteres Quartil, $x_{3/4}=$ oberes Quartil

\section{Streuungsmaße}
\paragraph{Definition}
Eine Abbildung $\sigma \colon \R^n \rightarrow \R$ mit $$\sigma(x_1+a,\ldots,x_n+a) = \sigma(x_1,\ldots,x_n)\text{ (Translationsinvarianz)}$$ heißt \textbf{Streuungsmaß}.

\subsection{Empirische Varianz}
$s^2 := \frac{1}{n-1} \sum\limits_{j=1}^{n}{(x_j - \bar{x})^2}$ = \textbf{empirische Varianz} von $x_1,\ldots,x_n$

\subsection{Empirische Standardabweichung}
$s := + \sqrt{s^2}$ = \textbf{empirische Standardabweichung} von $x_1,\ldots,x_n$

\subsection{Spannweite}
$x_{(n)} - x_{(1)}$ = \textbf{Spannweite} von $x_1,\ldots,x_n$

\subsection{Quartilsabstand}
$x_{(3/4)} - x_{(1/4)}$ = \textbf{Quartilsabstand} von $x_1,\ldots,x_n$

\section{Empirischer Korrelationskoeffizient}

$(x_1,y_1), \ldots, (x_n,y_n) \in \R^2$
TODO: BILD

Gesucht: Gerade $y = a + b \cdot x$ so, dass
$$(*) \sum\limits_{j=1}^{n}{(y_j - a - b x_j)^2} \overset{a,b}\rightarrow \text{Min}$$

\paragraph{Definition}
$\sigma_{x}^2 = \frac{1}{n}\sum\limits_{j=1}^{n}{(x_j - \bar{x})^2}$
$\sigma_{y}^2 = \frac{1}{n}\sum\limits_{j=1}^{n}{(y_j - \bar{y})^2}$

$\sigma_{xy} = \frac{1}{n}\sum\limits_{j=1}^{n}{(x_j - \bar{x})(y_j - \bar{y})}$ \textbf{empirische Kovarianz} $\sigma_x^2 > 0, \sigma_y^2 >0.$

Lösung von (*):
$b^* = \frac{\sigma_{xy}}{\sigma_{x^2}}, a^*= \bar{y} - b^* \cdot \bar{x}$


$\min\limits_{a,b} {\sum\limits_{j=1}^{n}{(y_j - a - b x_j)^2}} \stackrel{!}{=} \min\limits_{b}{\sum\limits_{j=1}^{n}{(y_i - \bar{y} - b (x_j - \bar{x}))^2}}=\ldots$

"lineare Regression"
\newline

Einsetzen von $a^*$ und $b^*$ in die Zielfunktion:
$$0 \leq \sum\limits_{j=1}^{n}{(y_j - a^* - b^* x_j)^2} = \ldots = n \sigma_y^2 (1-(\frac{\sigma_{xy}}{\sigma_x \sigma_y})^2)$$

\paragraph{Definition}
$r_{xy}:= \frac{\sigma_{xy}}{\sigma_x \sigma_y}$ heißt \textbf{empirischer Korrelationskoeffizient} (\emph{Pearson}).

\paragraph{Folgerung}
$|r_{xy}|\leq 1$
\newline
Es gilt $r_{xy} = \pm 1 \Leftrightarrow$ Punktewolke liegt exakt auf der Geraden $y=a^*+b^*x$.
Dabei ist $b^* > 0$, falls $r_{xy} = 1$ und $b^* < 0$, falls $r_{xy} = -1$.
\newline 
\emph{Dieser empirische Korrelationskoeffizient ist ein Maß für die (affin) lineare Abhängigkeit zwischen den $x_j$ und den $y_j$.}

\chapter{Ereignisse und Zufallsvariablen}

\section{Definition}
Gegeben sei eine \underline{Grundmenge} $\Omega$. Die Elemente von $\Omega$ heißen \textbf{Elementarereignisse}. Teilmengen von $\Omega$ heißen \textbf{Ereignisse}. (Idee: $\omega \in \Omega$ ist Ausgang eines zufälligen Versuchs.)

\paragraph{Interpretation}
Ein Ereignis $A \subset \Omega$ "tritt ein", wenn $\omega \in A$.

\section{Beispiele}
\begin{itemize}
	\item (i) (Münzwurf) \newline
		$\Omega = \{0,1\} (\text{oder } \Omega = \{W,Z\})$
	\item (ii) (m Münzwürfe) \newline
		$\Omega = \{0,1\}^m (A = \{\omega = (\omega_1, \ldots, \omega_m) : \sum\limits_{j=1}^{m}{\omega_j} \geq k\} \text{ Ereignis })$
	\item (iii) Werfen von 2 Würfeln \newline
		$\Omega = \{1, \ldots, 6\}^2$
	\item (iv) Brownsche Bewegung \newline
		(TODO: BILD) Bewegung eines Blütenpollens in einer Flüssigkeit
		\newline
		$\Rightarrow$ Zukunftsmusik
		\newline
		$\Omega = C([0,1], \R^2)$
\end{itemize}

\section{Bemerkung (Mengentheoretische Operationen)}
Seien $A, B, A_1, A_2, \ldots \subset \Omega$.
\newline
$A \cap B = \{\omega \in \Omega : \omega \in A \text{ und } \omega \in B\} \mathop{\hat{=}}  \text{"A und B treten ein"}$
\newline
$A \cup B \mathop{\hat{=}}  \text{"A oder B treten ein"}$
\newline
$\bar{A} \equiv A^c := \Omega \backslash A = \{\omega \in \Omega : \omega \notin A \} \mathop{\hat{=}} \text{"A tritt nicht ein"}$
\newline
$A \backslash B = A \cap B^c \mathop{\hat{=}} \text{"A tritt ein, aber nicht B"}$
\newline
$A \subset B \mathop{\hat{=}} \text{"wenn A, dann B"}$
\newline
$\emptyset \mathop{\hat{=}} \text{"unmögliches Ereignis"}$
\newline
$\Omega \mathop{\hat{=}} \text{"sicheres Ereignis"}$
\newline
\paragraph{Abkürzung} $AB = A \cap B$

\section{Definition}
Eine Abbildung $X \colon \Omega \rightarrow \R$ heißt (reelle) \textbf{Zufallsvariable}. Für $\omega \in \Omega$ heißt $X(\omega)$ \textbf{Realisierung} der Zufallsvariable zu $\omega$.

\paragraph{Idee} 
Mit $\omega \in \Omega$ bekommt auch $X(\omega)$ einen zufälligen Charakter.

\paragraph{Definition}
$X^{-1} \colon \mathcal{P}(\R) \rightarrow \mathcal{P}(\Omega) = \{A \colon A \in \Omega\}$ ist definiert durch 
$$X^{-1}(A) = \{\omega \in \Omega \colon X(\omega) \in A\} \text{ ("Urbild von A unter X")}$$

\paragraph{Bemerkung}
\begin{itemize}
\item $X^{-1}(A \cap B) = X^{-1}(A) \cap X^{-1}(B), A,B \subset \R$
\item $X^{-1}(A \cup B) = X^{-1}(A) \cup X^{-1}(B)$
\item $X^{-1}(\bigcup\limits_{j=1}^{\infty}{A_j}) = \bigcup\limits_{j=1}^{\infty}{X^{-1}(A_j)}$
\item $X^{-1}(\bigcap\limits_{j=1}^{\infty}{A_j}) = \bigcap\limits_{j=1}^{\infty}{X^{-1}(A_j)}$
\end{itemize}

\paragraph{Vereinbarung}
Es sei $X$ eine Zufallsvariable und $t \in \R$. Wir setzen
\begin{itemize}
 \item $\{X=t\} := \{\omega \colon X(\omega) = t \} (= X^{-1}(t))$
 \item $\{X \geq t\} := \{\omega \colon X(\omega) \geq t \}$
\end{itemize}

\section{Definition}
Sind $X,Y$ Zufallsvariablen, so definiert man
\begin{itemize}
	\item $(X+Y)(\omega) = X(\omega) + Y (\omega)$
	\item $(X-Y)(\omega) = X(\omega) - Y (\omega)$
	\item $(X \cdot Y)(\omega) = X(\omega) \cdot Y (\omega)$
\end{itemize}
$\omega \in \Omega, \text{neue Zufallsvariablen } X+Y, X-Y, X \cdot Y$
\newline
analog für $a \in \R$
\begin{itemize}
	\item $a X(\omega) = a \cdot (X(\omega))$
	\item $\min(X,Y) = (X \wedge Y)(\omega):= \min \{X(\omega), Y(\omega)\} \ldots$
\end{itemize}

\section{Definition}
Sei $A \subset \Omega$. Die Funktion $1_A \colon \Omega \rightarrow \R$ ist definiert durch
$$1_A(\omega) = \begin{cases} 1 & \text{, falls } \omega \in A \\
							  0 & \text{, falls } \omega \notin A						
\end{cases}
$$
und heißt \textbf{Indikatorfunktion} von $A$.

\section{Bemerkungen (Rechenregeln für Indikatorfunktionen)}

\begin{itemize}
	\item $1_{\emptyset} \equiv 0$
	\item $1_{\Omega} \equiv 1$
	\item $(1_A)^2 = 1_A$
	\item $1_{A^c} = 1 - 1_A$
	\item $1_{A \cap B} = 1_A \cdot 1_B$
	\item $1_{A \cup B} = 1_A + 1_B - 1_{A \cap B}$
	\item $A \subset B \Leftrightarrow 1_A \leq 1_B$
	\item $1_{A \triangle B} = |1_A - 1_B|$
\end{itemize}

\section{Definition}
Seien $A_1, \ldots, A_n \subset \Omega$. Die Zufallsvariable 
$$X := \sum\limits_{j=1}^{n}{1_{A_j}}$$ heißt \textbf{Zählvariable} oder \textbf{Indikatorsumme}.

\paragraph{Bemerkung}
\begin{itemize}
	\item $\{X=0\} = \{\omega \colon X(\omega) = 0 \} = A_{1}^c \cap \ldots A_{n}^c$
	\item $\{X = n \} = A_1 \cap \ldots \cap A_n$
	\item $\{X = k\} = $ "genau k der Ereignisse $A_1, \ldots, A_n$ treten ein" = $\bigcup\limits_{T \subset \{1, \ldots, n\}, |T|=k}{\bigl (\bigcap\limits_{j \in T}{A_j} \cap \bigcap\limits_{j \notin T}{A_{j}^c} \bigr )}$ \newline
$(T \subset \{1, \ldots, n\}, |T| = \text{card } T = k)$
\end{itemize}

\chapter{Diskrete Wahrscheinlichkeitsräume}

\section{Motivation}
Zufallsexperiment mit Ausgängen in $\Omega$
\newline
n-malige, `unabhängige' Wiederholung
\newline
$\Rightarrow$ Ergebnis $(a_1, \ldots, a_n) \in \Omega^n$
\newline
$r_n(A):= \frac{1}{n} \sum\limits_{j=1}^{n}{1_A(a_j)}, A \subset \Omega$ relative Häufigkeit von $A$
\newline
$0 \leq r_n(A) \leq 1, r_n(\emptyset) = 0, r_n(\Omega) = 1$
\newline
$r_n(A \cup B) = r_n(A) + r_n(B), A \cap B = \emptyset$
\newline
empirisches Gesetz über Stabilisierung relativer Häufigkeiten:
\newline
$r_n(A) \underset{n \rightarrow \infty}{\leadsto} ? $

\section{Definition}
	Ein Paar $(\Omega, \mathbb{P})$ bestehend aus einer diskreten Menge $\Omega \neq \emptyset$ und einer Funktion $\mathbb{P} \colon \mathcal{P} \rightarrow \R$ heißt \textbf{diskreter Wahrscheinlichkeitsraum}, falls:
	\begin{itemize}
		\item (P1) $\mathbb{P}(A) \geq 0, A \subset \Omega$
		\item (P2) $\mathbb{P}(\Omega) = 1$
		\item (P3) $\mathbb{P}(\bigcup\limits_{j = 1}^{\infty}{A_j}) = \sum\limits_{j=1}^{\infty}{\mathbb{P}(A_j)}, A_i \cap A_j = \emptyset, i \neq j$
		\newline
		Diese Eigenschaft heißt $\sigma$-Additivität.
	\end{itemize}
	Man nennt $\mathbb{P}$ \textbf{Wahrscheinlichkeitsmaß (auf $\Omega$)} (oder Wahrscheinlichkeitsverteilung) und $\mathbb{P}(A)$ heißt \textbf{Wahrscheinlichkeit von A}.
	
\section{Folgerung}
\begin{itemize}
	\item a) $\Prim(\emptyset) = 0$
	\item b) $\Prim(\bigcup\limits_{j=1}^{n}{A_j}) = \sum \limits_{j=1}^{n}{\Prim(A_j)}, A_i \cap A_j = \emptyset, i \neq j$
	\item c) $0 \leq \Prim(A) \leq 1, A \subset \Omega$
	\item d) $\Prim(A \cup B) = \Prim(A) + \Prim(B) - \Prim(A \cap B), A,B \subset \Omega$
	\item e) $A \subset B \Rightarrow \Prim(A) \leq \Prim(B)$ (Monotonie)
	\item f) $\Prim(A^c) = 1 - \Prim(A)$ (Komplementärwahrscheinlichkeit)
	\item g) $\Prim(\bigcup\limits_{j=1}^{\infty}{A_j}) \leq \sum \limits_{j=1}^{\infty}{A_j}$ (Subadditivität)
	\item h) $A_n \subset A_{n+1}, n \in \N \Rightarrow \Prim(\bigcup\limits_{n=1}^{\infty}{A_n}) = \lim \limits_{n \rightarrow \infty}{\Prim(A_n)}$ (Stetigkeit von unten)
	\item i) $A_n \supset A_{n+1}, n \in \N \Rightarrow \Prim(\bigcap\limits_{n=1}^{\infty}{A_n}) = \lim \limits_{n \rightarrow \infty}{\Prim(A_n)}$ (Stetigkeit von oben)
\end{itemize}

\paragraph{Beweis}
$\bullet$ a): $A_j = \emptyset, j \in \N$ (P3) \footnote{$\Prim(\emptyset) = \Prim(\emptyset \cup \emptyset) = \Prim(\emptyset) + \Prim(\emptyset) = 2 \cdot \Prim(\emptyset)$} $\Prim(\emptyset) = 0$.
\newline
$\bullet$ b): $A_{n+1} = A_{n+2} = \ldots = \emptyset$ in P3!
\newline
$\bullet$ c) + f): Für $A \subset \Omega$ gilt nach b) (für n = 2): \newline
	$1 = \Prim(\Omega) = \Prim(A \cup A^c) \overset{(b)}{=} \Prim(A) + \Prim(A^c)$
\newline
$\bullet$ d): Nach b) gilt $\Prim(A) = \Prim(A \backslash B) + \Prim(A \cap B)$, $\Prim(B) = \Prim(B \backslash A) + \Prim(A \cap B)$ und somit $\Prim(A) + \Prim(B) - \Prim(A \cap B) = \Prim(A \backslash B) + \Prim (B \backslash A) + \Prim(A \cap B) \overset{(b)}{=} \Prim(A \cup B)$
\newline
$\bullet$ e): Wegen $B = A \cup (B \backslash A)$ folgt\footnote{(aus der Additivität)}
	$\Prim(B) = \Prim(A) + \Prim(B \backslash A) \geq \Prim(A)$
\newline
$\bullet$ g): $B_1 := A_1, B_2 := A_2 \backslash A_1, \ldots , B_n := A_n \backslash (\bigcup \limits_{j=1}^{n-1}{A_j}), n \geq 2$.
\newline
Dann gilt $B_n \subset A_n$ und $\bigcup\limits_{j=1}^{n}{B_j} = \bigcup\limits_{j=1}^{n}{A_j}$ sowie $B_i \cap B_j = \emptyset, i \neq j.$
\newline
Es folgt aus (P3):
\newline
$\Prim(\bigcup\limits_{j=1}^{\infty}{A_j}) \overset{!}{=} \Prim(\bigcup\limits_{j=1}^{n}{B_j}) \overset {(P3)} {=} \sum\limits_{j=1}^{n}{\Prim(B_j)} \overset {e)}{\leq} \sum\limits_{j=1}^{n}{\Prim(A_j)}$ ($\infty$ ist zugelassen)
\newline
$\bullet$ h) + i): Übungsaufgabe

\section{Satz}
Seien $A_1, \ldots, A_n \subset \Omega.$ Setze
$$S_k := \sum\limits_{1 \leq i_1 < \ldots < i_k \leq n}{\Prim(A_{i_1} \cap \ldots \cap A_{i_k})}$$
Dann gilt
\begin{itemize}
	\item a) $\Prim(\bigcup\limits_{j=1}^{n}{A_j}) = \sum\limits_{k=1}^{n}{(-1)^{k-1} S_k}$ `Siebformel'
	\item b) $\Prim(\bigcup\limits_{j=1}^{n}{A_j}) \leq \sum\limits_{k=1}^{2s+1}{(-1)^{k-1}S_k}, s = 0, \ldots, \lfloor \frac{n-1}{2} \rfloor$
	\newline
	$\Prim(\bigcup\limits_{j=1}^{n}{A_j}) \geq \sum\limits_{k=1}^{2s}{(-1)^{k-1}S_k}, s = 1, \ldots, \lfloor \frac{n}{2} \rfloor$
\end{itemize}

\paragraph{Beweisidee für Siebformel}
vollständige Induktion nach $n$:
\newline
$\text{\underline{n=2:} }\Prim(A_1 \cup A_2) \overset{(d)}{=} \Prim(A_1) + \Prim(A_2) - \Prim(A_1 \cap A_2) = S_1 - S_2$
\newline
$\text{\underline{n=3:} }\Prim(\underbrace{A_1 \cup A_2} \cup A_3) \overset {(d)} {=} \Prim(A_1 \cup A_2) + \Prim(A_3) - \Prim((A_1 \cup A_2) \cap A_3)$ \footnote{$(A_1 \cup A_2) \cap A_3 = (A_1 \cap A_3) \cup (A_2 \cap A_3)$}
\newline
  $\overset {(d)} {=} \Prim(A_1) + \Prim(A_2) - \Prim(A_1 \cap A_2) + \Prim(A_3) - \Prim(A_1 \cap A_3) - \Prim(A_2 \cap A_3) + \Prim(A_1 \cap A_2 \cap A_3) = S_1 - S_2 + S_3$
  
\section{Definition + Satz}
a) Sei $(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum. Dann heißt $p \colon \Omega \rightarrow \R$ definiert durch $p(\omega) := \Prim(\{\omega\})$ \textbf{Wahrscheinlichkeitsfunktion} (von $\Prim$).
\newline
Es gilt $\Prim(A) = \sum\limits_{\omega \in A}{p(\omega)}, A\subset \Omega$.
\newline
b) Sind $\Omega$ diskret und $p \colon \Omega \rightarrow \R$ eine Abbildung mit $p(\omega)\geq 0$ und $\sum\limits_{\omega \in \Omega}{p(\omega)} = 1$, so erhält man vermöge $\Prim(A):= \sum\limits_{\omega \in A}{p(\omega)}$ einen diskreten Wahrscheinlichkeitsraum.

\paragraph{Beweis}
$\bullet$ a) $\sigma$-Additivität ($A = \bigcup\limits_{\omega \in A}{\{\omega\}}$)
\newline
$\bullet$ b) $\sigma$-Additivität: Großer Umordnungssatz! (Analysis)

\section{Definition}
$|\Omega| =: n < \infty$. Definiere $\Prim(A)= \frac{|A|}{n}.$
Dann heißt $(\Omega, \Prim)$ (ein diskreter Wahrscheinlichkeitsraum!) \textbf{Laplace-Raum}. Man nennt $\Prim$ \textbf{Gleichverteilung} auf $\Omega$. 
\newline
(`homogene Münze', `Würfeln', \ldots)

\section{Definition}
Sei $\Omega \neq \emptyset$ beliebig!
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum $\Leftrightarrow \exists$ abzählbare Menge $\Omega_0 \subset \Omega$, $\exists p \colon \Omega \rightarrow [0, \infty)$ mit $p(\omega=0)$ für alle $\omega \notin \Omega_0$, und $\sum\limits_{\omega \in \Omega_0}{p(\omega)} = 1$, und $\Prim(A) = \sum \limits_{\omega \in A \cap \Omega_0}{p(\omega)}, A \subset \Omega$.

\paragraph{Wiederholung}
$(\Omega, \Prim)$ Wahrscheinlichkeitsraum
\newline
$p \colon \Omega \rightarrow [0,1], \sum\limits_{\omega \in \Omega}{p(\omega)} = 1$
\newline
$\Prim(A):=\sum\limits_{\omega \in A}{p(\omega)}, A \subset \Omega$
\newline
$p(\omega) := \Prim(\{\omega\})$
\newline
\hrule
\vspace{10pt}
$\Omega$ allgemeine Menge, $\Omega_0 \subset \Omega$ diskret
\newline
$p \colon \Omega \rightarrow [0,1], \sum\limits_{\omega \in \Omega_0}{p(\omega)}=1, p(\omega) = 0, \omega \notin \Omega_0$
\newline
$\Prim(A):= \sum\limits_{\omega \in A}{p(\omega)}:=\sum\limits_{\omega \in A \cap \Omega_0}{p(\omega)}$
\newline
$\Omega_0$= Träger von $\Prim$

\section{Definition}
\label{defVerteilung}
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum  mit Träger $\Omega_0$.
Es sei $X \colon \Omega \rightarrow \R$ eine Zufallsvariable.
Dann heißt die Funktion $\Prim^X \colon \Prim(\R) \rightarrow \R$ definiert durch 
$\Prim^X(B):= \Prim\left(X^{-1}(B)\right), B \subset \R$ \textbf{Verteilung um $X$}.

\section{Satz}
In der Situation von Definition \ref{defVerteilung} ist $(\R,\Prim^X)$ ein diskreter Wahrscheinlichkeitsraum mit Träger $B_0 := X(\Omega_0) = \{X(\omega) \colon \omega \in \Omega_0 \}$ 

\begin{proof}
Für $B \subset \R$.
\newline
$$\Prim^X(B) = \Prim(\{ \omega \colon X(\omega) \in B\})$$
$$\overset{!}{=} \Prim(\{\omega \colon X(\omega) \in B \cap B_0 \})$$
Definiert man für $t \in \R$
$$p_t = \Prim(\{\omega \colon X(\omega) = t\}) = \Prim(X = t)$$
so ergibt sich aus der $\sigma$-Additivität von $\Prim$
$$\Prim^X(B) = \sum\limits_{t \in B \cap B_0}{\Prim(\{\omega \colon X(\omega) = t \})} = \sum \limits_{t \in B \cap B_0}{p_t}$$
\end{proof}

\chapter{Kombinatorik}
$|A| = card(A)$ = Anzahl der Elemente einer endlichen Menge $A$

\section{Grundregeln}
$A_1, \ldots, A_k$ endliche Menge
\newline
(i) $A_i \cap A_j = \emptyset, i \neq j \Rightarrow \left|\bigcup\limits_{j=1}^{n}{A_j}\right| = \sum\limits_{j=1}^{n}{A_j}$
\newline
\label{grundregeln}
(ii) $|A_1 \times \ldots \times A_n| = \prod\limits_{j=1}^{k}{|A_j|}$

\section{Satz}
\label{urnen}
Es sollen k-Tupel $(a_1, \ldots, a_k)$ durch sukzessives Festlegen von $a_1,a_2,\ldots,a_k$ nach folgenden Regeln gebildet werden:
\begin{itemize}
	\item es gibt $j_1$ Möglichkeiten für die Wahl von $a_1$
	\item es gibt (dann) $j_2$ Möglichkeiten für die Wahl von $a_2$
	\item \ldots
	\item es gibt (dann) $j_k$ Möglichkeiten für die Wahl von $a_k$
\end{itemize}
Dann gibt es genau $j_1 \cdot \ldots \cdot j_k$ solcher Tupel.

\section{Beispiel (Urnenmodelle)}
Betrachte Urne mit $n$ durchnummerierten Kugeln.Es werden $k$ Kugeln nach folgenden Regeln gezogen: ($M:=\{1,\ldots,n\}$)
\newline
\begin{tabular}{|c|p{4cm}|p{4cm}|} \hline 
 \begin{tiny} \backslashbox{Zurücklegen\\ (Wiederholung)}{Beachtung der\\ Reihenfolge} \end{tiny} & ja & nein \\ \hline
ja & $k$-Permutationen aus $M$ mit Wiederholung, $Per_k^n$ & $k$-Kombinationen aus $M$ mit Wiederholung, $Kom_k^n$ \\ \hline
nein & $k$-Permutationen aus $M$ ohne Wiederholung, $Per_{k,\neq}^n$ & $k$-Kombinationen aus $M$ ohne Wiederholung, $Kom_{k,\neq}^n$ \\ \hline
\end{tabular}

\section{Definition} $M=\{1, \ldots,n\} (n \in \N)$
\begin{itemize}
	\item $Per_k^{n} := M^k$
	\item $Per_{k,\neq}^{n} := \{(a_1,\ldots,a_k) \in M^k \colon a_i \neq a_j \text{ für } i \neq j\}$
	\item $Kom_k^{n} := \{(a_1,\ldots,a_k= \in M^k \colon a_1 \leq a_2 \leq \ldots \leq a_k\}$
	\item $(Kom_{k,\neq}^{n} := \{(a_1,\ldots,a_k) \in M^k \colon a_1 < a_2 < \ldots < a_k\}$
\end{itemize}

\section{Satz}
\begin{itemize}
	\item (i) $|Per_k^n| = n^k$
	\item (ii) $|Per_{k,\neq}^n| := n^{\underline{k}} = n \cdot (n-1) \cdot \ldots \cdot (n-k+1)$
	\item (iii) $|Kom_k^n| = {{n+k-1} \choose {k}}$
	\item (iv) $|Kom_{k,\neq}^n| = {n \choose k}$
\end{itemize}

\begin{proof}
	(i): \ref{grundregeln}.(ii)
	\newline
	(ii) Satz \ref{urnen}
	\newline
	(iv) Betrachte Äquivalenzrelation $$(a_1,\ldots,a_k) \sim (b_1,\ldots,b_k) \Leftrightarrow \{a_1,\ldots,a_k\} = \{b_1,\ldots,b_k\}$$ auf $Per_{k,\neq}^n$. Jede Äquivalenzklasse hat $k!$ Elemente! Es folgt
		$$|Kom_{k,\neq}^n| \cdot k! = |Per_{k,\neq}^n| = n^{\underline{k}}$$
	(iii) Die Abbildung $g \colon Kom_k^n \rightarrow Kom_{k,\neq}^{n+k-1}$ definiert durch $$(a_1,\ldots,a_k) \mapsto (a_1,a_2+1,a_3+2,\ldots,a_k+k-1)$$ ist eine Bijektion! (Umkehrabbildung!) Es folgt(!)
		$$|Kom_k^n| = |Kom_{k,\neq}^{n+k-1}| = {{n+k-1} \choose k}$$
\end{proof}

\section{Beispiel (Geburtstagsproblem)}
Wie groß ist die Wahrscheinlichkeit, dass unter $k$ rein zufällig ausgewählten Personen mindestens zwei am selben Tag Geburtstag haben?

\paragraph{Antwort}
Betrachte $\Sigma = Per_k^n$ mit $n=365$, und der Laplace-Verteilung.
\newline
Es sei $A:=\left\{(a_1,\ldots,a_k) \in \Omega \colon \text{ es gibt } i,j \in \{1,\ldots,k\} \text{ mit } i \neq j, a_i = a_j \right \}$.
Es gilt
$$\Prim(A) = 1- \Prim(A^c)$$
$$= 1- \Prim(Per_{k,\neq}^n)$$
$$\overset{!}{=}1-\frac{|Per_{k,\neq}^n|}{card \Omega}$$
$$= 1- \frac{n \cdot (n-1) \cdot \ldots \cdot (n-k+1)}{n^k}$$
$$= 1- \frac{n}{n} \cdot \frac{(n-1)}{n} \cdot \ldots \cdot \frac{n-k+1}{n}$$
$$= 1- (1-\frac{1}{n}) \cdot \ldots \cdot (1- \frac{k-1}{n})$$
\underline{$k=23$:} $\Prim(A) \approx 0,507 > \frac{1}{2}$
\newline
$n={49 \choose 6}, k=4004, \Prim(A) = 0,5001 > \frac{1}{2}$

\section{Beispiel}
$n$ Personen bringen (zu einer Feier) je ein Geschenk mit. Geschenke werden "rein zufällig" verteilt. Mit welcher Wahrscheinlichkeit bekommt mindestens eine Person ihr eigenes Geschenk? 
\newline
$\leadsto$ Siebformel!

\section{Beispiel (Besetzungsmodelle)}
$k$ Teilchen sollen auf $n$ nummerierte Fächer verteilt werden. Analogie zu Urnenmodell: Nummer der Kugel $\hat{=}$ Nummer des Fachs, Nummer der Ziehung $\hat{=}$ Nummer des Teilchens

\begin{tabular}{|c|p{3cm}|p{3cm}|} \hline 
 \begin{tiny} \backslashbox{Unterscheidbare\\ Teilchen}{Mehrfachbesetzungen} \end{tiny} & ja & nein \\ \hline
ja & $Per_k^n$ \textcolor{blue}{Maxwell-Boltzmann} & $Kom_k^n$ \textcolor{blue}{Bose-Einstein-Statistik}\\ \hline
nein & $Per_{k,\neq}^n$ \textcolor{blue}{Fermi-Dirak-Statistik} & $Kom_{k,\neq}^n$ \\ \hline
\end{tabular}
\textcolor{blue}{Statistische Physik}

\chapter{Der Erwartungswert}
$p(\omega) = \Prim(\{\omega\})$, $(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum

\section{Definition}
\begin{itemize}
	\item Der Erwartungswert einer Zufallsvariablen $X \colon \Omega \rightarrow \R$ existiert (genauer: $X$ ist integrierbar bezüglich $\Prim$), falls
	\begin{equation}
		\label{ewert}
		\sum\limits_{\omega \in \Omega}{|X(\omega)| p(\omega)} < \infty
	\end{equation}
	In diesem Fall heißt 
		$$\E X = \E[X] := \sum\limits_{\omega \in \Omega}{X(\omega)p(\omega)}$$
		(Physik: $<X>=\E[X]$) \textbf{Erwartungswert von $X$}.
	\item Ist $X \geq 0$ eine Zufallsvariable, so heißt
		$$\E X := \sum\limits_{\omega \in \Omega}{X(\omega)p(\omega)} \in [0,\infty]$$
		ebenfalls Erwartungswert von $X$.
\end{itemize}

\section{Satz}
Sei $L^1 \equiv L^1(\Prim) := \{X \colon X \text{ erfüllt } \ref{ewert} \}$. Dann ist $L^1$ ein reeller Vektorraum. Genauer:
\begin{itemize}
	\item (i) $\E [X+Y] = \E X + \E Y, X,Y \in L^1$
	\item (ii) $\E [aX] = a \E X, X \in L^1, a \in \R$
	\item (iii) $\E 1_A = \Prim(A), A \subset \Omega$
	\item (iv) $X \leq Y \Rightarrow \E X \leq \E Y$
	\item (v) $|\E X| \leq \E |X|$
\end{itemize}

\begin{proof}
	(i) $|(X+Y)(\omega)| \leq |X(\omega)| + |Y(\omega)|$.
	\newline
	Also $X+Y \in L^1(\Prim)$ und
	$$\sum\limits_{\omega \in \Omega}{(X(\omega)+Y(\omega)) p(\omega)} \overset{!}{=} \sum\limits_{\omega \in \Omega}{X(\omega)p(\omega)} + \sum\limits_{\omega \in \Omega}{Y(\omega)p(\omega)}$$
	(ii) analog
	\newline
	(iii) $\E 1_A = \sum\limits_{\omega \in \Omega}{p(\omega)}=\Prim(A)$
	\newline
	(iv) + (v) Übungsaufgabe
\end{proof}

\section{Folgerung}
\label{folg}
Seien $A_1, \ldots, A_n \subset \Omega$ und $X := \sum\limits_{j=1}^{n}{1_{A_j}}$. Dann gilt $\E X = \sum\limits_{j=1}^{n}{\Prim(A_j)}$. \newline
(Gilt auch für $\infty$ viele Ereignisse.)

\section{Satz (Transformationsformel)}
Seien $X \colon \Omega \Rightarrow \R$ und $g \colon \R \rightarrow \R$. Definiere $g(X) \colon \Omega \rightarrow \R$ durch $$g(X)(\omega) = g(X(\omega)).$$ Dann ist $g(X) \in L^1(\Prim)$ genau dann, wenn
$$\sum\limits_{x \colon \Prim(X=x)>0}{|g(x)| \Prim(X=x)} < \infty$$\footnote{$\Prim(X=x) = \Prim(\{\omega \in \Omega \colon X(\omega)=x\})$}

In diesem Fall gilt
$$\E g(x) = \sum\limits_{x \colon \Prim(X=x)>0}{g(x) \Prim(X=x)}$$

\begin{proof}
	Es gilt
	$$\sum\limits_{\omega \in \Omega}{|g\left(X(\omega)\right)|p(\omega)} = \sum\limits_{x \in \R \colon \Prim(X=x)>0}{|g(x)|} \sum\limits_{\omega \in \Omega \colon X(\omega) = x}{p(\omega)}$$
	\hrule
	$\Omega = \bigcup\limits_{x \in \R \colon X(\omega) = x, \Prim(X=x)>0}{\{\omega \colon X(\omega) = x\}} \cup \Omega^\prime, \Prim(\Omega^\prime) = 0$
	\hrule
	$$= \sum{|g\left(X(\omega)\right)|p(\omega)} = \sum\limits_{\omega \notin \Omega^\prime}{|g\left(X(\omega)\right)|p(\omega)}$$
	$$= \sum\limits_{x \in \R \colon \Prim(X=x)>0}{\sum\limits_{\omega \in \{\omega \in \Omega \colon X(\omega) = x \}}{|g(X(\omega))|p(\omega)}}$$
	$$= \sum\limits_{x \cdots}{|g(x)| \Prim(X=x)}$$
	Ist das endlich, so gilt die Rechnung auch ohne Betragsstriche!
	\newline
	Insbesondere gilt
	$$\E X = \sum\limits_{x \in \R}{x \Prim(X=x)} \left(g(x)\equiv x\right)$$
\end{proof}

\section{Beispiele}
\begin{itemize}
	\item Würfelwurf, $X$=Augenzahl, $\Prim(X=j)=\frac{1}{6}$.
		\newline Also $$\E X = \sum\limits_{j=1}^{6}{j \cdot \Prim(X=j)}=\frac{6\cdot 7}{2} \cdot \frac{1}{6} = \frac{7}{2} = 3,5$$
	\item Zweifacher Würfelwurf, $X$=Maximum der Augenzahlen ($\Omega=\{1,\ldots,6\}^2, \Prim = \text{Gleichverteilung}$)
		 $$\Prim(X=1)=\frac{1}{36}$$
		 $$\Prim(X=2)=p((1,2)) + p((2,1)) + p((2,2)) = \frac{3}{36}$$
		 \underline{Allgemein:} $\Prim(X=j)=\frac{2j-1}{36}, j =1,\ldots,6$
		 \newline
		 Es folgt
		 $$\E X = \sum\limits_{j=1}^{6}{j \cdot \frac{2j-1}{36}} \overset{?}{\approx} 4,47$$
\end{itemize}

\chapter{Die hypergeometrische Verteilung und die Binomialverteilung}
Urne mit Kugeln $\underbrace{1,2,\ldots,r}_{rot}, \underbrace{r+1,\ldots,r+s}_{schwarz}$
\newline $r,s \in \N_0, r+s > 0$.

\section{Definition}
\begin{itemize}
	\item $n$ mal Ziehen ohne Zurücklegen
	\item $a_j :=$ Nummer der $j$-ten gezogenen Kugel
	\item $\Omega = Per_{n,\neq}^{r+s}$
	\item $\Prim =$ Gleichverteilung ("unabhängiges", "rein zufälliges" Ziehen)
	\item $A_j := \{(a_1,\ldots,a_n) \in \Omega \colon a_j \leq r\} \hat{=} \{\text{j-te gezogene Kugel ist rot}\}$
	\item $X:= \sum\limits_{j=1}^{n}{1_{A_j}}$= Anzahl der gezogenen roten Kugeln
\end{itemize}
$\Prim^X$ (die Verteilung von $X$) heißt \textbf{hypergeometrische Verteilung} mit Parametern $r,s,n$, kurz:
$$X \sim Hyp(n,r,s), n \leq r+s$$
$$\Prim^X = Hyp(n,r,s)$$

\section{Satz}
Es gilt
\begin{itemize}
	\item (i) $\E X = n \cdot \frac{r}{r+s}$
	\item (ii) $\Prim(X=k) = \frac{{r \choose s}{s \choose {n-k}}}{{{r+s} \choose {n}}}, k = 0, \ldots, r \wedge n$
\end{itemize}

\begin{proof}
	(i) Es gilt (Symmetrieargument!) $|A_j| = r \cdot (r+s-1)^{\underline{n-1}}$
	\newline
	$|\Omega| = (r+s)^{\underline{n}}$
	$\Rightarrow \Prim(A_j) = \frac{|A_j|}{|\Omega|} = \frac{r}{r+s}$
	\newline
	Aus \ref{folg} folgt $\E X = n \cdot \frac{r}{r+s}$
	\newline
	(ii) $|\{X=k\}| \overset{!}{=} {n \choose k} r^{\underline{k}} s^{\underline{n-k}}$
	\newline
	$\Rightarrow \Prim(X=k) = \frac{{n \choose k} r^{\underline{k}} s^{\underline{n-k}}}{(r+s)^{\underline{n}}} = \frac{{r \choose k}{s \choose {n-k}}}{{{r+s}\choose n}}$
\end{proof}

\section{Motivation}
$X$ Zufallsvariable, $\sum\limits_{k=1}^r{\Prim(X=x_n)} = 1$
\newline
$X_1, X_2, \ldots, X_n$ "unabhängige" Wiederholungen von $X$ (= Ergebnis eines zufälligen Versuchs)
\newline
$\bar{X}:=\frac{1}{n}(X_1 + \ldots + X_n)$ Zufallsvariable!
\newline
Mit $h_j := card\{i \in \{1,\ldots,n\} \colon X_i = x_j \}$ gilt $\bar{X} \overset{!}{=} \frac{1}{n}(h_1x_1 + h_2x_2 + \ldots + h_nx_n)$
\newline
empirisches Gesetz über Stabilität relativer Häufigkeiten
\newline
$\underset{n \rightarrow \infty}{\rightarrow} \Prim(X=x_1)x_1 + \ldots + \Prim(X=x_r)x_r \overset{!}{=} \E X$
\newline
$X \sim Hyp(n,r,s) = \Prim^X, n \leq r +s$
\newline
$\Prim(X=k)=\frac{
			{{r \choose k}{s \choose {n-k}}}}{{{r+s}\choose n}}, k = 0, \ldots, n$
\newline
Wegen ${m \choose l}:= 0$ für $m<l$ gilt:
$\Prim(X=k)=0$ für $k < r$ und für $n-k>s (k < n-s)$

\section{Definition}
\textbf{Binomialverteilung}:
\begin{itemize}
	\item n maliges Ziehen aus einer Urne mit $r+s$ Kugeln mit Zurücklegen
	\item $\Omega = Per_n^{r+s} = \{(a_1,\ldots,a_n) \colon 1 \leq a_i \leq r+s, i = 1, \ldots, n\}$
	\item $\Prim$ Gleichverteilung
\end{itemize}
$X:= \sum\limits_{j=1}^n {1_{A_j}}, A_j := \{(a_1,\ldots,a_n) \in \Omega \colon a_j \leq r\}$
\newline
$\Prim^X$ heißt Binomialverteilung mit Parametern n und $p:=\frac{r}{r+s}$. Man schreibt auch $Bin(n,p):=\Prim^X$.

\section{Satz}
Es gilt
\begin{enumerate}
	\item $\E X = np$
	\item $\Prim(X=k)= {n \choose k} p^k (1-p)^{n-k}, 0 \leq k \leq n$
\end{enumerate}

\begin{proof}
\begin{enumerate}
	\item $|A_j|=r \cdot (r+s)^{n-1}$
		\newline
		$|\Omega| = (r+s)^n \leadsto \Prim(A_j) = \frac{|A_j|}{|\Omega|}= \frac{r}{r+s}=p$
		\newline
		Folgerung 5.3 $\leadsto \E X = np$.
	\item $card\{X=k\} = {n \choose k} r^k s^{n-k}$
		\newline
		$\leadsto \Prim(X=k)=\frac{{n \choose k} r^k s^{n-k}}{(r+s)^k (r+s)^{n-k}}$
\end{enumerate}
\end{proof}

\paragraph{Bemerkung}
$Bin(n,p)$ ist für jedes $p \in [0,1]$ definiert.

\chapter{Mehrstufige Experimente}
\section{Beispiel}
Urne mit einer roten und drei schwarzen Kugeln
\newline
\underline{1. Experiment} Kugel ziehen, Farbe notieren, Kugel und eine weitere Kugel derselben Farbe zurücklegen
\newline
\underline{2. Experiment} Erneut Kugel ziehen
\newline
Modell: $\Omega := \{0,1\} \times \{0,1\}, \quad (0 \hat{=} s, 1 \hat{=} r)$
\paragraph{Konstruktion von $\Prim$}
$p(\omega):= \Prim(\{\omega\})$

$\left.
\begin{array}{cc} % für mehrzeiligen Text nötig
p(1,1) := \frac{1}{4} \cdot \frac{2}{5} = \frac{2}{20} = \frac{1}{10}
 \\ p(1,0) := \frac{1}{4} \cdot \frac{3}{5} = \frac{3}{20} 
 \\ p(0,1) := \frac{3}{4} \cdot \frac{1}{5} = \frac{3}{20} 
 \\ p(0,0) := \frac{3}{4} \cdot \frac{4}{5} = \frac{12}{20}
\end{array}
\right\}
\text{1. Pfadregel}
$
$$\sum\limits_{\omega \in \Omega}{p(\omega)} = 1.$$
Betrachte $B:= \{(1,1),(0,1)\}$.
Dann gilt
$$\Prim(B) = p(1,1) + p(0,1) = (\text{2. Pfadregel})$$
$$= \frac{2}{20} + \frac{3}{20} = \frac{1}{4} \overset{!}{=} \Prim(\text{erste Kugel ist rot})$$
(TODO: Bild(Baumdiagramm))

\section{Definition}
\paragraph{Mehrstufige Experimente}
$\Omega = \Omega_1 \times \ldots \times \Omega_n$ ($\Omega_j \hat{=}$ Grundraum für $j$-tes Teilexperiment)
\newline
$\omega = (a_1, \ldots, a_n) \in \Omega$
\newline
Problem: Definiere $p(\omega) = \Prim(\{\omega\})$
\begin{enumerate}
	\item Startverteilung $p_1 \colon \Omega_1 \rightarrow [0,1] \quad \sum\limits_{\omega \in \Omega_1}{p_1(\omega)} = 1$
	\item Übergangswahrscheinlichkeiten $p_2(a_2 | a_1) \geq 0 \quad \sum\limits_{a_2 \in \Omega_2}{p_2(a_2|a_1)} \overset{!}{=} 1$ 
	
	$(p_2(a_2|a_1) \hat{=}$ Wahrscheinlichkeit, dass 2. Versuch das Ergebnis $a_2$ liefert unter der Bedingung, dass 1. Versuch Ergebnis $a_1$ geliefert hat.)
	\newline
	$p_3(a_3|a_1,a_2) \geq 0 \quad \sum\limits_{a_3 \in \Omega_3}{p_3(a_3|a_1,a_2)} = 1$
	\newline	
	...
	\newline
	$p_n(a_n | a_1, \ldots, a_{n-1}) \geq 0 \quad \sum\limits_{a_n \in \Omega_n}{p_n(a_n|a_1, \ldots, a_{n-1})} = 1$
\end{enumerate}
Setze für $\omega = (a_1, \ldots, a_n) \in \Omega$
$$p(\omega) := p_1(a_1) \cdot p_2(a_2|a_1) \cdot p_3(a_3|a_1,a_2) \cdot \ldots \cdot p_n(a_n|a_1,\ldots,a_{n-1}) \quad \text{1. Pfadregel}$$
Schließlich sei
$$\Prim(A) := \sum\limits_{\omega \in A}{p(\omega)}, \quad A \subset \Omega \qquad \text{Produkt von Übergangswahrscheinlichkeiten}$$

\section{Satz}
$(\Omega, \Prim)$ ist diskreter Wahrscheinlichkeitsraum.

\begin{proof}
	zu zeigen: $\sum\limits_{\omega \in \Omega}{p(\omega)} = 1$
	\newline
	Induktion (oder direkt)! Zum Beispiel gilt für $n=2$
	$$\sum\limits_{\omega \in \Omega}{p(\omega)} = \sum\limits_{(a_1,a_2) \in \Omega_1 \times \Omega_2}{p_1(a_1) p_2(a_2|a_1)} = \sum\limits_{a_1 \in \Omega_1}{\sum\limits_{a_2 \in \Omega_2}{p_1(a_1) p_2(a_2|a_1)}}$$
	$$\sum\limits_{a_1 \in \Omega_1}{p_1(a_1) \cdot 1} = 1.$$
\end{proof}

\section{Beispiel}
\paragraph{Unabhängige Experimente}
$(\Omega_j,\Prim_j), j=1,\ldots,n$, diskrete Wahrscheinlichkeitsräume, $p_i(a_i)=\Prim_i(\{a_i\})$
\newline
Idee: "Unabhängiges" Durchführen der zugehörigen Experimente
$$\Omega := \Omega_1 \times \ldots \times \Omega_n, p(\omega):= p_1(a_1) \cdot \ldots \cdot p_n(a_n), \omega=(a_1, \ldots, a_n) \in \Omega$$
$\big(p_2(a_2|a_1) = p_2(a_2), \ldots, p_n(a_n|a_1, \ldots, a_{n-1})=p_n(a_n)\big)$

$$\Prim(A) = \sum\limits_{\omega \in A}{p(\omega)}$$
Man nennt $\Prim$ das \textbf{Produkt} von $\Prim_1, \ldots, \Prim_n$ und schreibt 
	$$\Prim := \bigotimes\limits_{i=1}^n{\Prim_i}.$$
	z.B. kann $\Omega = \Omega_1 \times \Omega_2, \quad \Omega_1 = \Omega_2 = \{1,\ldots,6\}$
	\newline
	$p_1(a_1)=p_2(a_2)=\frac{1}{6}$
	\newline
	Dann ist 
		$$p(a_1,a_2)=\frac{1}{6} \cdot \frac{1}{6} = \frac{1}{36}$$
		und $\Prim$ ist die Laplace-Verteilung auf $\Omega$.
		
\chapter{Bedingte Wahrscheinlichkeiten}
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum.
\section{Definition}
Sei $B \subset \Omega$ mit $\Prim(B)>0$. Dann heißt
$$\Prim(A|B):=\frac{\Prim(A \cap B)}{\Prim(B)}$$
\textbf{bedingte Wahrscheinlichkeit} von $A \subset \Omega$ unter der Bedingung $B$.
\newline
Alternativ: $P_B(A):=\Prim(A|B)$

\section{Satz}
$P_B$ ist ein Wahrscheinlichkeitsmaß auf $\Omega$.
Dabei ist $P_B(A)=1$ falls $B \subset A$ und $P_B(A) = 0$ falls $A \cap B = \emptyset$.
Es gilt:
$$p_B(\omega):= \begin{cases}
		\frac{p(\omega)}{\Prim(B)} & \text{, falls } \omega \in B \\
			0 & \text{, sonst}
	\end{cases} \qquad \text{ mit } p_B(\omega):=\Prim_B(\{\omega\})$$
	Beweis ist klar! ($\sum\limits_{\omega \in \Omega}{p_B(\omega)} = \frac{1}{\Prim(B)} \sum \limits_{\omega \in B}{p(\omega)} = \frac{\Prim(B)}{\Prim(B)}=1$.)
	
\paragraph{Motivation}
Für $A \subset B$
$$\frac{h_n(A)}{h_n(B)}= \frac{\frac{1}{n}h_n(A)}{\frac{1}{n}h_n(B)} \leadsto \frac{\Prim(A)}{\Prim(B)}.$$

\section{Bemerkung (Zusammenhang zu Übergangswahrscheinlichkeiten)}
$\Omega = \Omega_1 \times \Omega_2, \quad p(\omega) = p_1(a_1)p_2(a_2|a_1), \quad \omega = (a_1,a_2)$
\newline
Für $a_1 \in \Omega_1$ sei
$$B := \{a_1\} \times \Omega_2.$$
Für $a_2 \in \Omega_2$ sei
$$A := \Omega_1 \times \{a_2\}.$$
Es gilt $A \cap B = \{(a_1,a_2)\}$,
$$\Prim(A \cap B) = \sum\limits_{\omega \in A\cap B}{p(\omega)} = \sum\limits_{(b_1,b_2) \in A \cap B}{p_1(b_1) p_2(b_2|b_1)} = p_1(a_1) p_2(a_2|a_1),$$
$$\Prim(B) = \sum\limits_{b_2 \in \Omega_2}{p(a_1|b_2)} = \sum\limits_{b_2 \in \Omega_2}{p_1(a_1) p_2(b_2|a_1)} = p_1(a_1)$$
Es folgt
$$\Prim(A|B) = \frac{\Prim(A \cap B)}{\Prim(B)} \overset{p_1(a_1)>0}{=} p_2(a_2|a_1)$$

\section{Satz (Multiplikationsformel)}
Seien $A_1, \ldots, A_n \subset \Omega$ mit $\Prim(A_1 \cap \ldots \cap A_{n-1}) > 0$.
Dann gilt
$$\Prim(A_1 \cap \ldots \cap A_n) = \Prim(A_1) \Prim(A_2|A_1) \Prim(A_3|A_1 \cap A_2) \cdot \ldots \cdot \Prim(A_n|A_1 \cap \ldots \cap A_{n-1})$$

\begin{proof}
	Für $n=2$:
		$$\Prim(A_1 \cap A_2) = \Prim(A_1) \cdot \Prim(A_2|A_1)$$
	\underline{Allgemein:} Ausschreiben der Definitionen + kürzen
	\newline
	$n=3$: rechte Seite: $\Prim(A_1) \cdot \frac{\Prim(A_1 \cap A_2)}{\Prim(A_1)} = \frac{\Prim(A_1 \cap A_2 \cap A_3)}{\Prim(A_1 \cap A_2)} = \Prim(A_1 \cap A_2 \cap A_3)$
\end{proof}

\section{Satz}
Sei $A_1, A_2, \ldots$ Zerlegung von $\Omega (\bigcup{A_i} = \Omega, A_i \cap A_j = \emptyset, i \neq j)$.
\newline
Sei $B \subset \Omega$. Dann gilt
\begin{enumerate}
	\item $\Prim(B) = \sum\limits_{j=1}^{\infty}{\Prim(A_j)\Prim(B|A_j)}$ Formel der totalen Wahrscheinlichkeit
	\item \footnote{Formel von Bayes} Für $\Prim(B)>0$, so gilt $$\Prim(A_k|B) = \frac{\Prim(A_k)\Prim(B|A_k)}{\sum\limits_{j=1}^\infty{\Prim(A_j)\Prim(B|A_j)}},\quad k=1,2,\ldots$$
\end{enumerate}
(Man vereinbart $\Prim(B|A_j)\Prim(A_j):= 0$, falls $\Prim(A_j)=0$)

\begin{proof}
	\begin{enumerate}
		\item $B = B \cap \Omega = \bigcup\limits_{j=1}^\infty{\underbrace{B \cap A_j}_{\text{paarweise disjunkt}}}$ 
		Aus der $\sigma$-Additivität von $\Prim$ folgt
		$$\Prim(B)=\sum\limits_{j=1}^\infty{\Prim(B \cap A_j)} = \sum\limits_{j=1}^\infty{\Prim(B|A_j)\Prim(A_j)}$$
		\item rechte Seite der Behauptung: $\frac{\Prim(B \cap A_k)}{\Prim(B)} \overset{!}{=} \Prim(A_k|B)$
	\end{enumerate}
\end{proof}

\section{Beispiel}
Eine Krankheit komme bei $4 \%$ der Bevölkerung vor\footnote{Die Mediziner sprechen von "Prävalenz".}. Ein Test spreche bei $90 \%$ der Kranken an und bei $20 \%$ der Gesunden!
\paragraph{Modell}
\begin{itemize}
	\item $\Omega :$ Menge der Personen in Deutschland
	\item $K \subset \Omega :$ Menge der kranken Personen
	\item $A \subset \Omega :$ Menge der (hypothetisch) positiv getesteten Personen
	\item $\Prim$ = Gleichverteilung auf $\Omega$
\end{itemize}

Dann 
$$\Prim(K|A)=\text{ Wahrscheinlichkeit, dass eine positiv getestete Person krank ist}$$
$$\overset{Bayes}{=} \frac{\Prim(K)\Prim(A|K)}{\Prim(K)\Prim(A|K) + \Prim(K^c)\Prim(A|K^c)} \quad (K = A_j, K^c = A_k)$$
$$= \frac{0,04 \cdot 0,9}{0,04 \cdot 0,9 + 0,96 \cdot 0,2} = \frac{0,036}{0,036 + 0,192} = \frac{0,036}{0,228} = 0,158$$

\section{Beispiel (Ziegenproblem)} Ausgelassen.

\section{Beispiel (Simpson-Paradoxon)}
Zulassung von Studenten in Berkeley (1973)
\begin{itemize}
	\item Zulassungsrate Männer: $44 \%$
	\item Zulassungsrate Frauen: $35 \%$
\end{itemize}
\underline{Aber:} Zulassungsraten der Männer in den einzelnen Fächern kleiner als die der Frauen

\paragraph{Erklärung}
\begin{itemize}
	\item $A \hat{=}$ Zulassung \footnote{Ereignis, dass rein zufällig ausgewählter Bewerber erfolgreich ist mit seiner Bewerbung.}
	\item $B \hat{=}$ Frau \footnote{Ereignis, dass zufällig ausgewählte weibliche Bewerberin erfolgreich ist.}
	\item $K_j \hat{=}$ Bewerbung für Fach $j$
\end{itemize}

Dann kann gelten
$$\Prim(A|B) < \Prim(A | B^c)$$
aber
$$\Prim(A|B \cap K_j) > \Prim(A|B^c \cap K_j), \quad j=1,2,\ldots$$
Denn:
$$\Prim(A|B)= \frac{\Prim(A\cap B)}{\Prim(B)} = \sum\limits_{j}{\frac{\Prim(A \cap B\cap K_j)}{\Prim(B)} \frac{\Prim(B \cap K_j)}{\Prim(B \cap K_j)}}$$
$$ = \sum\limits_{j}{\underbrace{\Prim(K_j|B)}_{\text{Bewerbungsrate der Frauen im j-ten Fach}}\underbrace{\Prim(A|B \cap K_j)}_{\text{siehe oben}}}$$
analog
$$\Prim(A|B^c) = \sum{\Prim(K_j|B^c) \Prim(A|B^c \cap K_j)}$$

Die absolute Erfolgsquote ist eine gewichtete Summe der relativen Erfolgsquoten.

\chapter{Stochastische Unabhängigkeit}
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum.

\section{Definition}
$A_1, \ldots, A_n \subset \Omega$ heißen \textbf{stochastisch unabhängig}, falls

$$\Prim(\bigcap\limits_{j \in T}{A_j}) = \prod\limits_{j \in T}{\Prim(A_j)}, \quad T \subseteq \{1, \ldots, n\}, |T| \geq 2.$$

($2^n - n - 1$ Gleichungen.)

\section{Bemerkung}
\begin{enumerate}
	\item $A,B$ stochastisch unabhängig \newline
	$\Leftrightarrow \Prim(A \cap B) = \Prim(A)\Prim(B)$ \newline
	$\overset{\Prim(B)>0}{\Leftrightarrow} \Prim(A|B) = \Prim(A)$ (Interpretation!)
	\footnote{Wenn die Kenntnis des Eintretens von B keinerlei Rückschlüsse auf das Eintreten von A zulässt.} \newline
	$\Leftrightarrow \Prim(B|A) \overset{\Prim(B)>0}{=} \Prim(B)$
	
	\item $\Prim(B) = 0 \leadsto$ $A$ und $B$ sind stochastisch unabhängig \newline
		$\Prim(B) = 1 \leadsto$ $A$ und $B$ sind stochastisch unabhängig
		
	\item $A,B,C$ unabhängig $\Leftrightarrow$
		
		$\left.
\begin{array}{cc} % für mehrzeiligen Text nötig
\Prim(A \cap B) = \Prim(A)\Prim(B)
 \\ \Prim(A \cap C) = \Prim(A)\Prim(C) 
 \\ \Prim(B \cap C) = \Prim(B)\Prim(C) 
\end{array}
\right\}
\text{paarweise stochastische Unabhängigkeit}
$
		
		$$\Prim(A \cap B \cap C) = \Prim(A) \Prim(B) \Prim(C)$$
\end{enumerate}

\paragraph{Wiederholung}
$A,B \subset \Omega$ stochastisch unabhängig $$\Leftrightarrow \Prim(A \cap B)=\Prim(A) \Prim(B)$$
$$\Leftrightarrow \Prim(A|B)=\Prim(A) \Leftrightarrow \Prim(B|A)=\Prim(B)$$
$A_1, \ldots, A_n$ stochastisch unabhängig $\Leftrightarrow$
$$\Prim(\bigcap\limits_{j \in T}{A_j}) = \prod\limits_{j \in T}{\Prim(A_j)}, \quad T \subset \{1, \ldots, n\}, |T| \geq 2$$

\section{Bemerkungen}
(iv) $A,B$ stochastisch unabhängig. Dann:
$$\Prim(A \cap B^c) = \Prim(A) - \Prim(A \cap B)$$
$$\overset{!}{=} \Prim(A) - \Prim(A) \Prim(B)$$
$$=\Prim(A)(1-\Prim(B)) = \Prim(A)\Prim(B^c)$$

Also sind $A$ und $B^c$ (also auch $A^c$ und $B^c$ bzw. $A^c$ und B) stochastisch unabhängig.
\newline
(v) Seien $A_1, \ldots, A_n$ unabhängig und $1 \leq i_1 < \ldots < i_k \leq n$. Dann sind $A_{i_1}, \ldots, A_{i_k}$ stochastisch unabhängig.
\newline
(vi) Ist $A$ von $A$ unabhängig, so ist 
$$\Prim(A)=\Prim(A)^2$$
d.h. $\Prim(A) \in \{0,1\}$.
\newline
(vii) Man nennt $A_1, A_2, \ldots \subset \Omega$ stochastisch unabhängig
$$\overset{d}{\Leftrightarrow} A_1, \ldots, A_n \text{ stochastisch unabhängig für jedes } n \geq 2.$$

\section{Beispiel (Produkträume)}
Sei $(\Omega, \Prim):= (\bigotimes\limits_{j=1}^{n}\Omega_j, \bigotimes\limits_{j=1}^n{\Prim_j}),$
d.h. $$\Prim(\{(a_1, \ldots, a_n)\})=p(\omega) \quad \omega=(a_1, \ldots, a_n)$$
$$= p_1(a_1) \cdot \ldots \cdot p_n(a_n) \quad (p_i(a_i)=\Prim_i(\{a_i\}))$$
Sei $B = B_1^* \times \ldots \times B_n^*, \quad B_i^* \in \Omega_i$.
Dann $\Prim(B_1^* \times \ldots \times B_n^*)$
$$= \sum\limits_{(a_1,\ldots,a_n)\in B_1^* \times \ldots \times B_n^*}{p_1(a_1) \cdot \ldots \cdot p_n(a_n)}$$
$$= \sum\limits_{a_1 \in B_1^*}{\ldots \sum\limits_{a_n \in B_n^*}{p_1(a_1)\cdot \ldots \cdot p_n(a_n)}}$$
$$= \prod\limits_{j=1}^n{\sum\limits_{a \in B_j^*}{p_j(a)}} = \prod\limits_{j=1}^n{\Prim_j(B_j^*)} \qquad (*)$$
Sei jetzt $A_j^* \subset \Omega_j, j=1,\ldots,n.$
\paragraph{Behauptung}
$A_j = \Omega_1 \times \ldots \times \Omega_{j-1} \times A_j^* \times \Omega_{j+1} \times \ldots \times \Omega_n, \quad j=1, \ldots, n$ stochastisch unabhängig.
\begin{proof}
	Sei $T \subset \{1,\ldots,n\}$ mit $|T| \geq 2$.
	\newline
	Definiere $$B_j^*:=\begin{cases}
		A_j^*, & j \in T, \\ \Omega_j, & j \notin T.
	\end{cases}$$
	Dann 
	$$\Prim(\bigcap\limits_{j \in T}{A_j}) = \Prim(B_1^* \times \ldots \times B_n^*)$$
	\footnote{$(A_1 \times A_2) \cap (B_1 \times B_2) = (A_1 \cap B_1) \times (A_2 \cap B_2)$}
	$$(*) \quad \prod\limits_{j=1}^n{\Prim_j(B_j^*)}= \prod\limits_{j \in T}{\Prim_j(A_j^*)}$$
	$$\overset{(*)}{=} \prod\limits_{j \in T}{\Prim_j(A_j)}$$ \footnote{mit $B_i^*=\Omega_i$ bis auf ein $i$}
\end{proof}

\section{Satz}
$A_1, \ldots, A_n$ stochastisch unabhängig $\Leftrightarrow$
$$\Prim(\bigcap\limits_{j \in I}{A_j} \cap \bigcap\limits_{j \in J}{A_j^c}) = \prod\limits_{j \in I}{\Prim(A_j)} \prod\limits_{j \in J}{\Prim(A_j^c)} \quad I,J \subset \{1, \ldots, n\}, I \cap J = \emptyset$$
(Hierbei $\bigcap\limits_{j \in \emptyset}{B_j}:= \Omega, \prod\limits_{j \in \emptyset}{a_j}:=1$)

\begin{proof}
	Induktion über Anzahl der Elemente von $J$ (vergleiche auch Bemerkung 9.3 (iv))
\end{proof}

\paragraph{Definition}
Für $A \subset \Omega$ sei $A^0 := A^c, A^1 := A$.
\newline
Für $B_1, \ldots, B_n \subset \Omega$ sei
$$\sigma(B_1, \ldots, B_k):=\{B \subset \Omega \colon \exists U \subset \{0,1\}^k \text{ mit } B = \bigcup\limits_{(\epsilon_1, \ldots, \epsilon_n) \in U}{B_1^{\epsilon_1} \cap \ldots \cap B_k^{\epsilon_k}}\}.$$
(Die von $B_1, \ldots, B_k$ erzeugte \underline{Algebra}).

\begin{example}
	(TODO: Bild)
\end{example}

\begin{remark}
	Eine Menge der Form
	$$B_1^{\epsilon_1} \cap \ldots \cap B_k^{\epsilon_k} \text{ für } (\epsilon_1, \ldots, \epsilon_k) \in \{0,1\}^k$$
	heißt \underline{Atom} von $\sigma(B_1, \ldots, B_k)$. Jede Menge in $\sigma(B_1, \ldots, B_k)$ ist Vereinigung von Atomen. Insbesondere gilt
	$$B_1, \ldots, B_k \in \sigma(B_1, \ldots, B_k), \emptyset \in \sigma(B_1, \ldots, B_k), \Omega \in \sigma(B_1, \ldots, B_k).$$
\end{remark}

\section{Satz (Blockungslemma)}
Seien $A_1, \ldots, A_k, A_{k+1}, \ldots, A_n$ stochastisch unabhängig und \newline $B \in \sigma(A_1, \ldots, A_k), C \in \sigma(A_{k+1}, \ldots, A_n)$. Dann sind $B$ und $C$ stochastisch unabhängig.

\begin{proof}
	Es gilt
	$$\Prim(B \cap C) = \Prim\left( \underbrace{(\bigcup\limits_{(\epsilon_1, \ldots, \epsilon_k) \in U}{A_1^{\epsilon_1} \cap \ldots A_k^{\epsilon_k}})}_{B} \cap \underbrace{(\bigcup\limits_{(\epsilon_{k+1}, \ldots, \epsilon_n) \in V}{A_{k+1}^{\epsilon_{k+1}} \cap \ldots \cap A_n^{\epsilon_n}})}_{C} \right)$$
	disjunkte Vereinigung
	$$= \sum\limits_{U}{\Prim\left((A_1^{\epsilon_1} \cap \ldots \cap A_k^{\epsilon_k}) \cap \bigcup\limits_{V}{(A_{k+1}^{\epsilon_{k+1}} \cap \ldots \cap A_n^{\epsilon_n})}\right)}$$
	$$= \sum\limits_{U,V}{\Prim(A_1^{\epsilon_1} \cap \ldots \cap A_k^{\epsilon_k} \cap A_{k+1}^{\epsilon_{k+1}} \cap \ldots A_n^{\epsilon_n})}$$
	$$=\sum\limits_{U,V}{\left(\underbrace{\prod\limits_{j=1}^k{\Prim(A_j^{\epsilon_j})}}_{\Prim(A_1^{\epsilon_1} \cap \ldots \cap A_k^{\epsilon_k})} \underbrace{\prod\limits_{j=k+1}^n{\Prim(A_j^{\epsilon_j})}}_{\Prim(A_{k+1}^{\epsilon_{k+1}} \cap \ldots \cap A_n^{\epsilon_n})}\right)}$$
	$$=\sum\limits_{(\epsilon_1, \ldots, \epsilon_k) \in U}{\Prim(A_1^{\epsilon_1} \cap \ldots \cap A_k^{\epsilon_k})} \sum\limits_{(\epsilon_{k+1}, \ldots, \epsilon_n) \in V}{\Prim(A_{k+1}^{\epsilon_{k+1}} \cap \ldots \cap A_n^{\epsilon_n})}$$
	$$\overset{!}{=} \Prim(B) \Prim(C)$$
\end{proof}

\section{Satz}
$A_1, \ldots, A_n \subset \Omega$ stochastisch unabhängig. Ferner gelte $\Prim(A_i)=p, \quad i = 1, \ldots, n$. Dann ist
$$X := \sum\limits_{j=1}^n{1_{A_j}}$$
$Bin(n,p)$-verteilt.

\begin{proof}
	Es gilt
	$$\{X=k\} = \bigcup\limits_{T \subseteq \{1, \ldots,n\}, |T|=k}{\left( \bigcap\limits_{j \in T}{A_j} \cap \bigcap\limits_{j \notin T}{A_j^c}\right )}$$
	disjunkte Vereinigung, also
	$$\Prim(X=k)=\Prim(\{X=k\}) = \sum\limits_{T \subseteq \{1, \ldots, n\}, |T| = k}{\Prim(\bigcap\limits_{j \in T}{A_j} \cap \bigcap\limits_{j \notin T}{A_j^c})}$$
	$$\overset{\text{Voraussetzung}}{=}\sum\limits_{T \subseteq \{1, \ldots, n\}, |T| = k}{p^k (1-p)^{n-k}} = {n \choose k} p^k (1-p)^{n-k}$$
\end{proof}

\section{Beispiel (Bernoulli-Kette der Länge $n$)}
$(\Omega, \Prim):= \bigotimes\limits_{j=1}^n{(\Omega_j, \Prim_j)}$
\newline
$\Omega_1=\ldots=\Omega_n = \{0,1\}$
\newline
$\Prim_j(\{1\}) = 1 - \Prim_j(\{0\}) = p, \quad j = 1, \ldots, n$
\newline
Die Ereignisse 
	$$A_j := \{(a_1, \ldots, a_n)\in \Omega \colon a_j = 1\}, \quad j=1, \ldots, n$$
	sind stochastisch unabhängig.
\newline
Ferner $\Prim(A_j)=p$.

\chapter{Gemeinsame Verteilung}
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum (mit Träger $\Omega_0$).

\section{Definition}
Seien $X_j \colon \Omega \rightarrow \R \quad (j = 1, \ldots, k)$ Zuvfallsvariablen. 
Definiere $X \colon \Omega \rightarrow \R^k$ vermöge $X(\omega) = \left(X_1(\omega), \ldots, X_k(\omega)\right), \quad \omega \in \Omega$.
Dann heißt $X$ \textbf{$k$-dimensionaler Zufallsvektor}.
\newline
Für $B \subset \R^k$ sei $X^{-1}(B) \equiv \{X \in B\} := \{\omega \in \Omega \colon X(\omega) \in B\}$. Die Abbildung
$$\Prim^X \colon \mathcal{P}(\R) \rightarrow [0,1]$$ definiert durch
$$\Prim^X(B):= \Prim(\{X \in B\}) (= \Prim(X \in B))$$ heißt
\underline{Verteilung} von $X$  oder auch gemeinsame Verteilung von $X_1, \ldots, X_k$.
\newline
Die Verteilung von $X_j$ heißt \textbf{$j$-te Marginalverteilung (von $X_j$)}.

%hier beginnt der 29.11.2011
\paragraph{Wiederholung}
$(\Omega, \Prim)$ Wahrscheinlichkeitsraum
\newline
$\Omega_0 := \{ \omega \in \Omega \colon \Prim(\{\omega\})>0 \}$ diskret
\newline
$X = (X_1, \ldots, X_k) \colon \Omega \rightarrow \R^k$
\newline
Verteilung: $\Prim^X$
$$\Prim^X(A) := \Prim(X \in A) \equiv \Prim(\{\omega \colon X(\omega) \in A\}), \quad A \subset \R^k$$

\paragraph{Bemerkung}
Die gemeinsame Verteilung legt Randverteilungen fest. ($k=2$)
$$\Prim(X_1 = x_1) \quad \left(=\Prim^{X_1}(\{x_1\})\right)$$
$$= \Prim\left(\{X_1 = x_1\} \cap \bigcup\limits_{x_2 \in X_2(\Omega_0)}{\{X_2 = x_2\}}\right)$$
$$\overset{\text{$\sigma$-Additivität}}{=} \sum\limits_{x_2  \in X_2(\Omega_0)}{\underbrace{\Prim(\{X_1 = x_1\} \cap \{X_2 = x_2\})}_{\Prim(X_1 = x_1, X_2 = x_2)}}$$
$$= \sum\limits_{x_2 \in X_2(\Omega_0)}{\Prim^{(X_1,X_2)}\left(\{(x_1,x_2)\}\right)}$$

\section{Beispiel}
$\Omega := \{1, \ldots, 6\}^2$
\newline
$\Prim$ = Gleichverteilung (2-maliger Würfelwurf)
$$X_1((k,l)):= \min(k,l), X_2((k,l)) := \max(k,l)$$
(TODO: Tabelle)
$$\Prim(X_1 = i, X_2 = j), \quad i,j = 1, \ldots, 6$$



Es gilt
$$\Prim(X_1 = i) = \Prim(X_2 = 7-i), \quad i=1,\ldots,6$$
$$\leadsto \Prim^{X_1} = \Prim^{7-X_2} \quad (X_1 \neq 7-X_2)$$
$$X_1 \overset{d}{=} 7-X_2 \quad \text{Verteilungsgleichheit}$$

\section{Beispiel}
$\Prim(X_1 = i, X_2 = j)$
\newline
$(c \in [0,\frac{1}{2}] \text{ fest})$

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline 
\backslashbox{i}{j} & 1 & 2 &  \\ 
\hline 
1 & c & $\frac{1}{2}-c$ & $\frac{1}{2}$ \\ 
\hline 
2 & $\frac{1}{2}-c$ & c & $\frac{1}{2}$ \\ 
\hline 
 & $\frac{1}{2}$ & $\frac{1}{2}$ &  \\ 
\hline 
\end{tabular} 
\end{center}

$$\Prim(X_1 = 1) = \Prim(X_1 = 1, X_2 = 1)+\Prim(X_1 = 1, X_2 = 2)$$

$$\leadsto \Prim^{X_1} = \Prim^{X_2} \hat{=} \text{ fairer Münzwurf!}$$
Also legen die Randverteilungen $\Prim^{X_1}, \Prim^{X_2}$ die gemeinsame Verteilung $\Prim^{(X_1,X_2)}$ \underline{nicht} fest.

\section{Definition}
$X_1, \ldots, X_k \colon \Omega \rightarrow \R$ heißen \textbf{stochastisch unabhängig}
$$\Leftrightarrow \{X_1 \in B_1\}, \ldots, \{X_k \in B_k\} \text{ stochastisch unabhängig } \forall B_1, \ldots, B_k \subset \R$$

\section{Satz}
Die folgenden Aussagen sind äquivalent:
\begin{enumerate}
	\item $X_1, \ldots, X_k$ sind stochastisch unabhängig
	\item $\Prim(X_1 \in B_1, \ldots, X_k \in B_k) = \prod\limits_{i=1}^k{\Prim(X_i \in B_i)} \quad B_1, \ldots, B_k \subset \R$
	\item $\Prim(X_1 = x_1, \ldots, X_k = x_k) = \prod\limits_{i=1}^k{\Prim(X_i = x_i)} \quad x_1, \ldots, x_k \in \R$
\end{enumerate}

\begin{proof}
	(1) $\Rightarrow$ (2) Klar nach Definition.
	\newline
	(2) $\Rightarrow$ (1): Wähle in der Definition $B_j = \R$ für $j \notin T (\{X_j \in B_j\} = \Omega)$
	\newline
	(2) $\Rightarrow$ (3): Setze $B_i = \{x_i\}$
	\newline
	(3) $\Rightarrow$ (2): Für $B_1, \ldots, B_k \subset \R$ gilt
		$$\Prim(X_1 \in B_1, \ldots, X_k \in B_k) \quad (=\Prim^{(X_1,\ldots,X_k)}(B_1 \times \ldots \times B_k)) \quad (B_i \subset \underbrace{X_i(\Omega_0)}_{\text{diskret}})$$
		$$= \sum\limits_{x_1 \in B_1, \ldots, x_k \in B_k}{\Prim(X_1 = x_1, \ldots, X_k = x_k)}$$
		$$\overset{(3)}{=} \sum{\Prim(X_1 = x_1) \ldots \Prim(X_k = x_k)}$$
		$$= \Prim(X_1 \in B_1) \cdot \ldots \cdot \Prim(X_k \in B_k)$$\footnote{$\sum\limits_{i,j}{a_ib_j} = (\sum{a_i})(\sum{b_j}) \text{ "Fubini"}$}
\end{proof}

\paragraph{Bemerkung}
Im Falle stochastischer Unabhängigkeit legen die Randverteilungen die gemeinsame Verteilung fest.

\section{Satz (Blockungslemma)}
Es seien $X_1, \ldots, X_k$ stochastisch unabhängige, eindimensionale Zufallsvariablen und $1 \leq l \leq k-1$, $g \colon \R^l \rightarrow \R$, $h \colon \R^{k-l} \rightarrow \R$. Dann sind 
$g(X_1, \ldots, X_l)$ und $h(X_{l+1}, \ldots, X_k)$ stochastisch unabhängig.

\begin{proof}
Übung!
\end{proof}

\section{Satz (allgemeine Transformations-Formel)}
Seien $Z \colon \Omega \rightarrow \R^k$ Zufallsvektor, $g \colon \R^k \rightarrow \R$. Setze
$$M := \{ z \in \R^k \colon \Prim(Z=z)>0\}.$$
Dann ist $g(Z)$ integrierbar\footnote{d.h. der Erwartungswert existiert.} genau dann, wenn
$$\sum\limits_{z \in M}{|g(z)| \cdot \Prim(Z=z)} < \infty$$
In diesem Fall ist der Erwartungswert
$$\mathbb{E} g(Z) = \sum\limits_{z \in M}{g(z) \cdot \Prim(Z=z)}$$

\begin{proof}
vgl. eindimensionalen Spezialfall.
\end{proof}

\section{Satz}
Seien $X, Y \colon \Omega \rightarrow \R$ stochastisch unabhängig.
Sind $X,Y$ integrierbar, so ist auch $X \cdot Y$ integrierbar und $\mathbb{E}(X \cdot Y) = (\mathbb{E} X)\cdot(\mathbb{E} Y)$.

\begin{proof}
	Setze $M := \{(x,y) \in \R^2 \colon \Prim(X=x, Y=y) > 0 \} = \{(x,y) \colon \Prim(X=x) > 0, \Prim(Y=y) > 0\}$.
	Dann 
	$$\E | X \cdot Y | =  \sum\limits_{\omega \in \Omega_0}{|X(\omega)| |Y(\omega)| \Prim(\{\omega\})}$$
	$$\overset{!}{=} \sum\limits_{(x,y) \in M}{|x| |y| \underbrace{\Prim(X=x, Y=y)}_{\Prim(X=x) \cdot \Prim(Y=y)}}$$
	$$= \underbrace{\left(\sum\limits_{x \colon \Prim(X=x)>0}{|x| \Prim(X=x)}\right)}_{< \infty} \cdot \underbrace{\left(\sum\limits_{y \colon \Prim(Y=y)>0}{|y| \Prim(Y=y)}\right)}_{< \infty}$$
	Dieselbe Rechnung "ohne Betragsstriche" liefert die behauptete Formel.
\end{proof}

\section{Satz (Faltungsformel)}
Sind $X,Y$ unabhängige reelle Zufallsvariablen, so gilt 
$$\Prim(X+Y = z) = \sum\limits_{x \in X(\Omega_0)}{\Prim(X=x) \Prim(Y=z-x)}, \quad z \in \R$$

\begin{proof}
	Ohne Unabhängigkeit gilt
	$$\Prim(X+Y=z) \overset{!}{=} \sum\limits_{X \in X(\Omega_0)}{\Prim(X=x, Y=z-x)}$$
\end{proof}

\section{Satz (Additionsgesetz für Binomialverteilungen)}
Seien $X \sim Bin(m,p), Y \sim Bin(n,p)$ stochastisch unabhängig. \newline
Dann ist $X+Y \sim Bin(m+n,p)$ $\quad \forall m,n \geq 1, p \in [0,1]$

\begin{proof}
	Es seien $A_1, \ldots, A_m, A_{m+1}, \ldots, A_{m+n}$ unabhängige Ereignisse mit $\Prim(A_i)=p$.
	Dann sind
	$$X^\prime := \sum\limits_{i=1}^m{1_{A_i}}, \qquad Y^\prime := \sum\limits_{i=m+1}^{m+n}{1_{A_i}}$$
	$Bin(m,p)$ bzw. $Bin(n,p)$ Binomialverteilt. (Bernoulli-Kette)
	\newline
	Nach Blockungslemma sind $X^\prime, Y^\prime$ stochastisch unabhängig!
	Außerdem
	$$X^\prime + Y^\prime = \sum\limits_{i=1}^{m+n}{1_{A_i}} \sim Bin(m+n,p)$$
	Aber aus $(X^\prime, Y^\prime) \overset{d}{=} (X,Y)$ folgt
	$$X^\prime + Y^\prime \overset{d}{=} X+Y, \text{ d.h. $X+Y \sim Bin(m+n,p)$}$$
	$$\Prim^{X^\prime + Y^\prime} = \Prim^{X+Y}$$
\end{proof}

\chapter{Varianz, Kovarianz, Korrelation}
$(\Omega, \Prim)$ diskreter Wahrscheinlichkeitsraum.

\section{Definition}
Falls $X$ Zufallsvariable und $\E X^2 < \infty$, so heißt
$$V(X)\equiv Var(X) := \E (X - \E X)^2$$
\textbf{Varianz} von $X$.

\section{Bemerkungen}
\label{bem112}
\begin{enumerate}
	\item Wegen $$|X| \leq 1 + X^2$$
		$$(X-a)^2 \leq X^2 + 2 |a| |X| + a^2$$
		ist $V(X)$ wohldefiniert.
	\item Gilt $\sum\limits_{i=1}^\infty{\Prim(X=x_i)} = 1$, so ist
		$$V(X) = \sum{(x_i - \E X)^2 \Prim(X=x_i)} \quad \text{(Transformationsformel)}$$
	\item Es gilt 
		$$V(X) = \E (X^2-2 \underbrace{(\E X)}_{\mu} X + \underbrace{(\E X)^2}_{\mu ^2})$$
		$$= \E X^2 - 2 \mu \E X + \mu ^2 \quad \text{(Linearität des Erwartungswertes)}$$
		$$= \E X^2 - \mu ^2 = \E X^2 - (\E X)^2$$
	\item Varianz kann als Trägheitsmoment interpretiert werden.
\end{enumerate}

\section{Satz}
Sei $\E X^2 < \infty$.
\begin{enumerate}
	\item $V(X) = \E(X-c)^2 - (\E X - c)^2, \quad c \in \R \text{ (Steiner-Formel)}$
	\item $V(X) = \min\limits_{c}{ \E(X-c)^2}$
	\item $V(aX + b) = a^2 V(X)$
	\item $V(X) = 0 \Leftrightarrow \exists a \in \R \colon \Prim(X=a) = 1.$
\end{enumerate}

\begin{proof}
	(1) $V(X)=$
		$$= \E ( \underbrace{X-c} + \underbrace{c-\E X})^2$$
		$$= \E(X-c)^2 + 2 \underbrace{\E (X-c)(c- \E X)} + (c - \E X)^2$$ \footnote{$\E c = c$}
		$$= \E(X-c)^2 - 2 \E (c - \E X)^2 + (c - \E X)^2$$
	(2) (1) $\leadsto \E (X-c)^2 = V(X) + (\E X -c)^2$
	\newline
	(3) $\E (aX+b - \E (aX + b))^2$
	$$= \E (aX + b - a \E X - b)^2$$
	$$= a^2 \E (X - \E X)^2$$
	(4) Bemerkung \ref{bem112}.(2) $\leadsto$ 
	$$V(X) = 0 \Leftrightarrow (x_i - \E X)^2 \Prim(X=x_i)=0$$
	$$\Leftrightarrow \forall i \text{ mit } \Prim(X= x_i) \text{ gilt } x_i = \E X$$
	$$\Leftrightarrow \text{ Es gibt nur ein $i_0$ mit } \Prim(X=x_{i_0})>0$$
	$$ \text{ Dann ist } \Prim(X=x_{i_0}) = 1, \text{ und } x_{i_0} = \E X.$$
\end{proof}

\section{Beispiel}
\begin{enumerate}
	\item $X = 1_A, \quad A \subset \Omega$.
		$$Var X = \E 1_A^2 - (\E 1_A)^2 = \E 1_A - (\E 1_A)^2$$
		$$= \Prim(A)-\Prim(A)^2 = \Prim(A) (1 - \Prim(A))$$
	\item $X= \sum\limits_{i=1}^n{1_{A_i}}, \quad A_i \subset \Omega_i, i=1, \ldots, n$
		$$V(X) = \E \left(\sum\limits_{i=1}^n{1_{A_i}}\right) \cdot \left(\sum\limits_{j=1}^n{1_{A_j}}\right) - (\E \sum{1_{A_i}})^2$$
		$$ = \sum\limits_{i=1}^n{\Prim(A_i)} + \sum\limits_{i \neq j}{\Prim(A_i \cap A_j)} - (\sum\limits_{i=1}^n{\Prim(A_i)})^2$$
		Es gelte etwa (für ein $c \geq -r, r, s \in \N$)
		$$\Prim(A_i) = \frac{r}{r+s} =: p$$
		$$\Prim(A_i \cap A_j) = \frac{r}{r+s} \frac{r+c}{r+s+c}, \quad i \neq j$$
		($c=-1$: Ziehen ohne Zurücklegen, $c=0 \hat{=}$ Ziehen mit Zurücklegen, $c>0$: Polyasches Urnenschema). Dann
		$$V\left(\sum\limits_{i=1}^n{X_i}\right) = n p (1-p) \cdot \left(1 + \frac{(n-1) \cdot c}{r+s+c}\right)$$
		(3) $X \sim Bin(n,p): V(x) = n p (1-p) \quad (c=0)$ \newline
		(4) $X \sim Hyp (n,r,s): V(X) = n p (1-p) \left(1 - \frac{(n-1)}{r+s-1}\right) \quad (c=-1)$
\end{enumerate}

\section{Definition}
$X$ heißt \underline{standardisiert}, wenn $\E X = 0$ und $V(X)=1$. Ist $X$ eine beliebige Zufallsvariable $( \E X^2 < \infty)$, so heißt (falls $V(X)>0$)
$$X^* = \frac{X - \E X}{\sqrt{V(X)}}$$
\textbf{Standardisierung von $X$}. (Es gilt $\E X^* = 0, V(X^*) = 1$)

\paragraph{Bemerkung}
$X \sim Bin(n,p),\quad X^* = \frac{X-np}{\sqrt{n p (1-p)}}$

\section{Satz (Tschebyschov-Ungleichung)}
Für jede Zufallsvariable $X$ mit $\E X^2 < \infty$ gilt
$$\Prim(|X-\E X| \geq c) \leq \frac{V(X)}{c^2}), \quad c > 0.$$

\begin{proof}
	Es sei (für gegebenes $c>0$)
	$$g(t) = \begin{cases} 1, & \text{falls } |t - \E X | \geq c \\ 0, & \text{sonst}\end{cases}, \quad t \in \R$$
	Ferner sei 
	$$h(t)= \frac{(t - \E X)^2}{c^2}$$
	(TODO: Bild)
	\newline
	Wegen $g \leq h$ ist $g(X) \leq h(X)$, also
	$$\underbrace{\E g(X)}_{= \Prim(|X - \E X| \geq c)} \leq \underbrace{\E h(X)}_{= \underbrace{\E \frac{(X- \E X)^2}{c^2}}_{= \frac{1}{c^2} V(X)}}$$
\end{proof}

\section{Definition}
Es gelte $\E X^2 < \infty, \E Y^2 < \infty$. Die Zahl
$$(Cov(X,Y)=) C(X,Y) := \E (X - \E X) (Y - \E Y)$$
heißt \textbf{Kovarianz zwischen $X$ und $Y$}.
Gilt $C(X,Y)=0$, so heißen $X$ und $Y$ \textbf{unkorreliert}.
Gilt $V(X)>0, V(Y)>0$, so heißt
$$\rho(X,Y):= \frac{C(X,Y)}{\sqrt{V(X)}\sqrt{V(Y)}}$$
\textbf{Korrelationskoeffizient zwischen $X$ und $Y$}.
(Wegen $|a \cdot b| \leq \frac{1}{2} (a^2 + b^2)$ ist $(X-\E X) (Y - \E Y) \in L^1(\Prim)$)\footnote{$L^1(\Prim) \leadsto$ integrierbar}.

\section{Satz}
\begin{enumerate}
	\item $C(X,Y) = \E X Y - (\E X \E Y)$
	\item $C(X,Y) = C(Y,X), \quad C(X,X)=V(X)$
	\item $C(aX+b,cY+d) = a \cdot c \cdot C(X,Y)$
	\item $X,Y$ unabhängig $\Rightarrow C(X,Y) = 0$
	\item $V(X,Y) = V(X)+ V(Y) + 2 C(X,Y)$
	\item $V(\sum\limits_{j=1}^n{X_j}) = \sum{V(X_j)} + \sum\limits_{i \neq j}{C(X,Y)}$
	\item $C(1_A, 1_B) = \Prim(A \cap B) - \Prim(A) \Prim(B)$
	\item $C(\sum\limits_{i=1}^n{X_i}, \sum\limits_{j=1}^m{Y_j}) = \sum\limits_{i,j}{C(X_i, Y_j)}$
	\item $\rho(aX+b, cY+d) = sgn(a \cdot c)\rho(X,Y)$
\end{enumerate}

\begin{proof}
	a),b),c) stimmt.
	\newline
	d) Satz 10.8.
	$$C(X,Y)=\E X \cdot Y - \E X \E Y = 0.$$
	\newline
	e) folgt aus f, f folgt aus h
	\newline
	h) linke Seite
	$$\E (\sum\limits_ {i}{X_i} - \sum\limits_{i} {\E X_i}, \sum\limits_{j}{X_j} - \sum\limits_{j}{\E X_j})$$
	$$= \E ( \sum\limits_{i}{(X_i - \E X_i)}) (\sum\limits_{j}{(X_j - \E X_j)})$$
	$$= \sum\limits_{i,j}{\E (X_i - \E X_i) \cdot (Y_j - \E Y_j)}$$
	i)
für $a,b \in \R, c, d \in \R$
$$\rho(aX+b, cY+d)$$
$$\overset{Def}{=} \frac{cov(aX+b,cY+d)}{\sqrt{Var(aX+b)\cdot Var(cY+d)}}$$
$$\overset{11.8.c}{=}\frac{ac Cov(X,Y)}{\sqrt{a^2 c^2}\sqrt{Var(X)\cdot Var(Y)}}$$
$$=sgn(ac) \cdot \rho(X,Y)$$
\end{proof}

\section{Folgerung}
Sind $X_1,\ldots,X_n$ unabhängig, so folgt
$$V(\sum\limits_{i=1}^n{X_i}) = \sum\limits_{i=1}^n{V(X_i)} \quad \text{(aus iv+vi(d+f))}$$

\section{Beispiel}
Für $X \sim Bin(n,p)$ gilt (nach Satz 9.6)
$$X \overset{d}{=} X_1 + \ldots + X_n$$
mit $X_1, \ldots, X_n$ unabhängig und identisch verteilt mit 
$$\Prim(X_i=1)=1-\Prim(X_i=0)=p \quad \text{für } i = 1, \ldots, n$$
Also ist nach 11.9 und 11.4.i
$$V(X)= \sum\limits_{i=1}^n{V(X_i)}$$
$$= n \cdot V(X_1) = n p (1-p)$$
in Übereinstimmmung mit 11.4.iii (siehe auch Übung).
Das folgende Resultat ist analog zu 1.6:

\section{Satz}
Gilt $\E X ^2 < \infty, \E Y^2 < \infty$ und $V(X) V(Y) > 0$, so folgt
$$\min\limits_{a,b}{\E (Y-a-bX)^2} = V(Y) (1- \rho ^2 (X,Y))$$
Die Minimalstelle $(a^*, b^*)$ ist gegeben durch 
$$b^* = \frac{C(X,Y)}{V(X)}, \quad a^* = \E Y - b^* \E X$$

\begin{proof}
	$\left(\text{allg. }\inf\limits_{x,y}{f(x,y)} = \inf\limits_{x}{\inf\limits_{y}{f(x,y)}} = \inf\limits_{y}{\inf\limits_{x}{f(x,y)}}\right)$
	\newline
	Es seien $a,b \in \R$ und $Z:= Y-bX$.
	Dann ist
	$$\E (Y-bX-a)^2 = \E (Z-a)^2$$
	$$\overset{11.3.i, Steiner}{=} V(Z) + \underbrace{(\E Z - a)^2}_{\geq 0}$$
	Also ist $a^* = \E Z = \E Y - b^* \E X$.
	\newline
	Es verbleibt die Aufgabe
	$$\min\limits_{b}{\underbrace{\E (Y - \E Y - b (X - \E X))^2}_{=: f(b)}}$$
	$$f(b) = Var(Y) - 2 b C(X,Y) + b^2 Var(X)$$
	$$=\footnote{\text{quadratische Ergänzung}}V(X) \underbrace{\left(b - \frac{C(X,Y)}{V(X)}\right)^2}_{\geq 0} + V(Y) - \frac{C(X,Y)^2}{V(X)}$$
	$$\Rightarrow b^* = \frac{C(X,Y)}{V(X)}$$
\end{proof}

\section{Folgerung}
\begin{enumerate}
	\item $C(X,Y)^2 \leq V(X) \cdot V(Y)$ (Cauchy-Schwarz-Ungleichung)
	\item $|\rho(X,Y)| \leq 1$
	\item $|\rho(X,Y)| = 1$
		$$\Leftrightarrow \exists a,b \in \R \colon \E (Y-a-bX)^2 = 0$$
		$$\Leftrightarrow \exists a,b \in \R \colon \Prim(Y-a-bX=0)=1$$
		$$(\Leftrightarrow Y=a+bX \quad \Prim-\text{fast sicher})$$
		insbesondere $\rho(X,Y)=1 \Rightarrow b>0$
		$$(b^* = \frac{C(X,Y)}{V(X}) = \rho(X,Y) \cdot \underbrace{\sqrt{\frac{V(Y)}{V(X)}}}_{>0}$$
		und $\rho(X,Y)=-1 \Rightarrow b < 0.$
\end{enumerate}

\section{Bemerkung}
Falls $\Prim(X=x_j, Y=y_j) = \frac{1}{n} \quad (1 \leq j \leq n)$ so
$$\E(Y - a -bX)^2 = \sum\limits_{j=1}^n{\frac{1}{n} \cdot (y_j - a - b x_j)^2}$$
$\leadsto$ Methode der kleinsten Quadrate $\leadsto$ Kapitel 1 (empirischer Korrelationskoeffizient)

\chapter{Wichtige diskrete Verteilungen}
\begin{enumerate}
	\item Binomialverteilung $Bin(n,p) \leadsto$ Kapitel 6, 9.6
	\item Hypergeometrische Verteilung $Hyp(n,r,s) \leadsto$ Kapitel 6
	\item Poisson-Verteilung $Po(\lambda)$
	\item Geometrische Verteilung $G(p)$
	\item Negative Binomialverteilung $Nb(r,p)$
	\item Multinomialverteilung $Mult(n,p_1, \ldots, p_s)$
\end{enumerate}

\section{Satz (Gesetz seltener Ereignisse)}
Sei $(p_n)_{n \geq 1}, 0 \leq p_n \leq 1$ eine Folge mit
$$n p_n \overset{n \rightarrow \infty}{\rightarrow} \lambda \quad (0 < \lambda < \infty)$$
Dann gilt
$$\underbrace{{n \choose k} p_n^k (1-p_n)^{n-k}}_{=\Prim(X_n = k)\text{ für }X_n \sim Bin(n, p_n)} \overset{n \rightarrow \infty}{\rightarrow} e^{-\lambda} \frac{\lambda^k}{k!}$$

\begin{proof}
	linke Seite:
	$$\frac{n!}{k!(n-k)!} \frac{1}{n^k} \cdot (n \cdot p_n)^k (1- \frac{n \cdot p_n}{n})^{n-k}$$
	$$\frac{1}{k!} \underbrace{\frac{n^k}{n^k}}_{\rightarrow 1} \underbrace{(n, p_n)^k}_{\rightarrow \lambda^k} \underbrace{(1- \frac{n \cdot p_n}{n})^{n-k}}_{\rightarrow e^{- \lambda}}$$
	$$\frac{n^k}{n^k} = \frac{n}{n} \cdot \frac{(n-1)}{n} \cdot \frac{(n-2)}{n} \cdot \ldots \cdot \frac{(n-k+1)}{n}$$
	und allgemein für $a_n \rightarrow 1$ gilt
	$$(1 + \frac{a_n}{n})^n \rightarrow e^a$$
\end{proof}

\section{Definition}
$$X \sim Po(\lambda) \Leftrightarrow \Prim(X=k)= e^k \frac{\lambda^k}{k!}, \quad k \in \N_0$$
(Poisson-Verteilung mit Parameter $\lambda, \lambda \in (0, \infty)$)
Also, falls $n \cdot p_n \rightarrow \lambda,$
$$Bin(n,p_n) \rightarrow Po(\lambda) \text{ im Sinne von 12.1}$$

\section{Satz}
\begin{enumerate}
	\item $X \sim Po(\lambda) \Rightarrow \E X = V(X) = \lambda$
	\item $X,Y$ unabhängig, $X \sim Po(\lambda), Y \sim Po(\mu)$
		$$\Rightarrow X+Y \sim Po(\lambda + \mu) \text{(Additivgesetz)}$$
\end{enumerate}

\begin{proof}
	\begin{enumerate}
		\item $$\E X = \sum\limits_{k=1}^\infty {k \cdot e^{- \lambda} \frac{\lambda^k}{k!}} = e^{-\lambda} \cdot \lambda \underbrace{\sum\limits_{k=1}^\infty{\frac{\lambda^{k-1}}{(k-1)!}}}_{= \sum\limits_{j=0}^\infty{\frac{\lambda^{j}}{j!}} = e^\lambda} = \lambda$$
		$$\E (X (X-1)) = \sum\limits_{k=2}^\infty{k(k-1)e^{-\lambda} \frac{\lambda^k}{k!}} = e^{-\lambda} \lambda^2 e^\lambda = \lambda^2$$
		
		$$Var(X) = \E X^2 - (\E X)^2$$
		$$= \E(X (X-1)) + \E X - \E X^2$$
		$$= \lambda^2 + \lambda - \lambda^2 = \lambda$$
		\item Faltungsformel (Übung!)
	\end{enumerate}
\end{proof}

\section{Definition und Satz}
Sei $0 < p < 1$.
$$X \sim G(p) :\Leftrightarrow \Prim(X=k) = (1-p)^k \cdot p, \quad k \in \N_0$$
(geometrische Verteilung mit Parameter $p$)
\newline
Es gilt
\begin{enumerate}
	\item $\E X = \frac{1}{p} -1$
	\item $V(X) = \frac{1-p}{p^2}$
\end{enumerate}
$X$ modelliert \underline{Anzahl der Nieten vor dem ersten Treffer} in einer unendlichen Bernoulli-Kette mit Trefferwahrscheinlichkeit $p$.

\begin{proof}
	$$\sum\limits_{k=0}^\infty{X^k} \equiv \frac{1}{1-x} \text{ auf } (-1,1)$$
	$$\Rightarrow \sum\limits_{k=1}^\infty{k \cdot x^{k-1}} = \frac{1}{(1-x)^2} \text{ für } |x|<1$$
	$$\Rightarrow \sum\limits_{k=2}^\infty{k(k-1)x^{k-2}} = \frac{2}{(1-x)^3}$$
	$$\E X = \sum\limits_{k=1}^\infty{k (1-p)^{k} \cdot p} = p (1-p) \sum\limits_{k=1}^\infty {k (1-p)^{k-1}}$$
	$$p (1-p) \frac{1}{(1-(1-p))^2}$$
	$$= \frac{1-p}{p}$$

	$$\E X (X-1) = p(1-p)^2 \underbrace{\sum\limits_{k=2}^\infty{k (k-1) (1-p)^{k-2}}}_{= \frac{2}{p^3}}$$
	$$= 2\left(\frac{(1-p)}{p}\right)^2$$

	$$\Rightarrow Var(X) = 2 \left(\frac{1-p}{p}\right)^2 + \frac{1-p}{p} - \left(\frac{1-p}{p}\right)^2$$	
	$$= \left(\frac{1-p}{p}\right)^2 + \frac{1-p}{p} = \left( \frac{1-p}{p} \right) \underbrace{\left(\frac{1-p}{p}+1\right)}_{=\frac{1}{p}}= \frac{1-p}{p^2}$$
\end{proof}

\section{Definition und Satz}
$X$ hat eine negative Binomialverteilung mit Parametern $r$ und $p$ ($r \in \N$ und $0 < p < 1$), falls gilt:
$$\Prim(X=k) = {k+r-1 \choose k} p^r (1-p)^k, \quad k \in \N_0$$
Es gilt 
$$\E X = r \cdot \frac{1-p}{p}$$
$$V(X) = r \cdot \frac{1-p}{p^2},$$
Notation: $X \sim Nb(r,p)$

\section{Satz}
$X_1, \ldots, X_r$ $\overset{u.i.v.}{\sim}$\footnote{unabhängig identisch verteilt} $G(p)$
$$\Rightarrow X_1 + \ldots + X_r \sim Nb(r,p)$$

\begin{proof}
	Faltungsformel (Übung)
\end{proof}

\section{Bemerkung}
\begin{enumerate}
	\item $X \sim Nb(r,p)$
		$$\Prim(X=k) = \frac{(k+r-1)^{k_\_}}{k!} p^r (1-p)^k$$
		$$= \frac{-r \cdot (-r-1) \cdot \ldots \cdot (-r-k+1)}{k!} (-1)^k p^r (1-p)^k$$
		$$= {-r \choose k} \cdot p^r (-(1-p))^k$$
		wobei ${-r \choose k} = \frac{-r^k}{k!} \quad \forall r \in \R$
\end{enumerate}

\end{document}
